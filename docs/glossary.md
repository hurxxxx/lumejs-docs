---
title: 용어 사전
---

이 페이지는 이 책에서 자주 나오는 용어를 “실무에서 통용되는 의미” 기준으로 정리한 **확장 용어 사전**이다.  
책 본문에 직접 등장하지 않더라도, 실무자가 AI를 제대로 이해하고 AX 전환을 추진하려면 알아야 하는 용어를 포함했다.

아래 용어 설명은 “위키 스타일(정의 → 핵심 이론 → 실무 포인트 → 학습 자료)”로 구성했다.  
각 장의 **용어 풀이** 섹션은 해당 장 맥락에서 더 실무적으로 설명할 수 있다.

## 1) 전환 / 조직

### AX(AI Transformation)
**한 줄 정의**: AI를 “도구 추가”가 아니라 **업무·조직·데이터·거버넌스가 함께 바뀌는 전환**으로 보는 관점.

**핵심 이론**: 기술은 빠르게 바뀌지만, 조직 성과는 “업무 구조(프로세스) + 책임 구조 + 측정”이 바뀔 때 생긴다. 그래서 AX는 모델/툴 자체보다 “업무 흐름에 기준을 넣고 운영 가능한 상태로 만드는 것”을 중심에 둔다.

**실무 포인트**
- “AI를 쓴다”가 아니라 “어떤 업무를 어떤 책임 구조로 바꾸는가”로 문제를 정의한다.
- 파일럿(PoC)에서 끝나지 않게, 로그/평가/중단(롤백) 기준을 최소로라도 만든다.

**관련 용어**: Copilot, Agentic, RACI, 거버넌스, KPI/ROI, LLMOps

### Copilot(보조형)
**한 줄 정의**: 사람의 작업을 돕는 형태의 AI 활용(초안 생성, 요약, 정리, 검색 보조).

**핵심 이론**: 보조형은 “사람-검토”가 기본 설계라서 리스크를 낮추기 쉽다. 대신 성과를 내려면 템플릿/검토 체크리스트/공유 규칙 같은 **운영 설계**가 필요하다.

**실무 포인트**
- 보조형의 ROI는 “시간 절감”뿐 아니라 “일관성/오류 감소/검토 속도”로 잡는 게 현실적이다.
- “최종 책임은 사람”을 문서화하면 도입 갈등이 크게 줄어든다.

### Agentic(에이전트형)
**한 줄 정의**: 목표를 달성하기 위해 여러 단계를 이어서 수행하려는 형태(계획, 툴 호출, 반복, 상태 유지 포함).

**핵심 이론**: 에이전트형은 생산성이 커질 수 있지만, 권한 부여(툴 호출) 때문에 **안전/감사/통제 비용**이 급격히 늘 수 있다. OWASP는 “과도한 자율성(Excessive Agency)”을 주요 위험으로 분류한다.

**실무 포인트**
- 에이전트형은 “권한/범위/중단 기준”을 먼저 정해야 한다.
- 보조형으로 기준을 만들고, 그 기준을 그대로 에이전트형에 이식하는 순서가 안정적이다.

**학습 자료**
- OWASP Top 10 for LLM Applications(v1.1): https://owasp.org/www-project-top-10-for-large-language-model-applications/

### RACI
**한 줄 정의**: 역할과 책임을 정리하는 표(R: 수행, A: 최종책임, C: 협의, I: 통보).

**핵심 이론**: “책임의 공백”이 있으면 AI 사고가 발생했을 때 대응이 늦어진다. 반대로 A(최종책임)만 명확해도 승인/중단 의사결정이 빨라진다.

**실무 포인트**
- 최소 R/A만이라도 문서화한다(특히 대외 영향이 있는 업무).

### KPI/ROI
**한 줄 정의**: KPI는 성과 지표, ROI는 투자 대비 성과(비용 대비 효과)다.

**핵심 이론**: AI는 “성능(quality)·비용(cost)·리스크(risk)”가 동시에 움직인다. ROI는 비용 절감만이 아니라 리스크 감소(사고 예방), 품질 개선(오류율 감소)도 포함해야 현실적이다.

**실무 포인트**
- “샘플 10건”으로 기준선을 먼저 만든 뒤 확산한다.

## 2) 모델 / 학습(이론 핵심)

### LLM(대규모 언어 모델)
**한 줄 정의**: 텍스트를 입력받아 “다음 토큰”을 예측하는 방식으로 출력 토큰을 생성하는 모델.

**핵심 이론**
- 많은 LLM은 Transformer 계열 구조를 기반으로 한다.
- LLM은 “정답을 검색”하기보다 “그럴듯한 다음 토큰”을 생성하므로, 근거가 없는 영역에서 환각이 발생할 수 있다.

**학습 자료**
- Transformer 원전(Attention Is All You Need): https://arxiv.org/abs/1706.03762

### Transformer / Attention
**한 줄 정의**: Transformer는 “Attention(주의)”만으로 시퀀스를 처리하는 신경망 구조다.

**핵심 이론**
- Attention은 입력의 각 토큰이 “다른 토큰을 얼마나 참고할지” 가중치를 학습하는 메커니즘이다.
- Transformer는 RNN처럼 순차적으로 처리하지 않아 병렬화가 쉽고, 대규모 학습에 유리하다.

**실무 포인트**
- 실무자는 수식을 몰라도 된다. 다만 “컨텍스트 윈도우 한계”와 “근거 없는 생성”의 본질을 이해해야 한다.

**학습 자료**
- Attention Is All You Need(2017): https://arxiv.org/abs/1706.03762
- Wikipedia 요약(개념 잡기): https://en.wikipedia.org/wiki/Attention_Is_All_You_Need

### 토큰(Token) / 토크나이저(Tokenization)
**한 줄 정의**: 토큰은 모델이 다루는 텍스트 단위이고, 토크나이저는 텍스트를 토큰으로 쪼개는 규칙/모델이다.

**핵심 이론**
- 가격/지연/컨텍스트 윈도우는 대부분 “토큰 수”로 결정된다.
- 같은 문장이라도 언어/기호/공백에 따라 토큰 수가 크게 달라질 수 있다.

**실무 포인트**
- 비용 예측은 “업무 대표 샘플”을 실제로 돌려서 평균 토큰을 측정하는 게 가장 정확하다.

### 컨텍스트 윈도우(Context Window)
**한 줄 정의**: 모델이 한 번의 요청에서 참고할 수 있는 입력(및 일부 출력 포함) 길이(토큰) 한계.

**핵심 이론**
- 컨텍스트 윈도우가 작으면 긴 문서/장기 대화에서 누락이 생긴다.
- 크다고 해서 무조건 좋진 않다. 비용과 지연이 같이 늘고, “중요한 정보”가 묻힐 수 있다.

**실무 포인트**
- 긴 문서는 “요약-요약(계층형)” 또는 RAG로 다루는 게 운영상 안전하다.

### 사전학습(Pretraining) / 미세조정(Fine-tuning)
**한 줄 정의**: 사전학습은 대규모 일반 데이터로 기본 언어 능력을 학습하고, 미세조정은 특정 목적/데이터로 추가 학습하는 것이다.

**핵심 이론**
- 많은 LLM 제품은 “사전학습 + 지시/선호 기반 추가 학습” 조합으로 만들어진다.
- 미세조정은 품질을 올릴 수 있지만, 데이터 품질/저작권/개인정보 문제가 같이 따라온다.

### Instruction Tuning / RLHF / DPO
**한 줄 정의**
- Instruction tuning: “지시를 따르는” 데이터로 지도학습(SFT)하는 방식
- RLHF: 사람 선호를 활용해 강화학습으로 정렬(alignment)하는 방식
- DPO: RLHF의 일부 과정을 단순화한 선호 최적화 방식(강화학습 없이 선호 데이터로 직접 최적화)

**핵심 이론**
- “모델이 똑똑해졌다”는 말은 종종 “지시를 잘 따르게 튜닝됐다”는 의미다.
- 정렬 방식이 달라지면 거절/안전 동작, 톤, 상세도, 위험 민감도가 달라질 수 있다.

**학습 자료**
- InstructGPT(RLHF 계열 설명): https://arxiv.org/abs/2203.02155
- DPO: https://arxiv.org/abs/2305.18290

## 3) 검색 / 지식(RAG 심층)

### 임베딩(Embedding)
**한 줄 정의**: 문장/문서를 숫자 벡터로 표현해 “의미가 가까운 것”을 찾게 하는 방식.

**핵심 이론**
- 임베딩은 동의어/표현 차이가 있어도 비슷한 의미를 가까이 배치하려고 학습된다.
- 검색 품질은 “임베딩 모델”뿐 아니라 “문서 품질/청킹(chunking)/질문 템플릿”에 크게 좌우된다.

**실무 포인트**
- 임베딩만 바꿔도 검색 품질이 크게 달라질 수 있으므로, 변경 시 회귀 테스트가 필요하다.

### 벡터 DB(Vector DB) / 유사도 검색
**한 줄 정의**: 임베딩 벡터를 저장하고, 가까운 벡터를 빠르게 찾는(근사 최근접) 엔진/DB.

**핵심 이론**
- 유사도 검색은 “완벽히 정확한 최적해”보다 “충분히 좋은 근사해”를 빠르게 찾는 방식이 일반적이다.
- 따라서 정확도뿐 아니라 지연/비용/운영 난이도를 함께 본다.

### RAG(Retrieval-Augmented Generation)
**한 줄 정의**: 검색으로 관련 문서를 찾고, 그 문서를 근거로 답을 생성하는 구조.

**핵심 이론**
- RAG는 환각을 줄이는 현실적인 방법이지만, “검색이 틀리면 답도 틀린다.”
- RAG의 본질은 모델이 아니라 **문서 체계/버전/승인** 같은 조직 운영에 있다.

**학습 자료**
- RAG 원전 논문: https://arxiv.org/abs/2005.11401

### 청킹(Chunking) / 리랭킹(Reranking) / 하이브리드 검색
**한 줄 정의**
- 청킹: 문서를 검색 가능한 단위로 나누는 것
- 리랭킹: 후보 문서를 더 정교한 모델로 다시 정렬하는 것
- 하이브리드 검색: 키워드(BM25 등) + 벡터 검색을 함께 쓰는 것

**핵심 이론**
- 청킹이 너무 작으면 근거가 끊기고, 너무 크면 검색이 둔해진다.
- 하이브리드는 “정확한 키워드”와 “의미 유사”를 동시에 잡는 전략이다.

**실무 포인트**
- RAG 품질 문제는 대개 “문서 최신성/버전/승인”에서 먼저 터진다(기술 이전에 운영).

## 4) 프롬프트 / 시스템(실무 품질)

### 시스템 프롬프트(System Prompt)
**한 줄 정의**: 모델의 역할/규칙을 상위에서 고정하는 지시.

**핵심 이론**
- 시스템 프롬프트는 “정책/역할/금지”를 정하고, 사용자 프롬프트는 “업무 요청”을 담는다.
- 운영 관점에서는 시스템 프롬프트도 “버전 관리 대상”이다.

**실무 포인트**
- 시스템 프롬프트 변경은 결과를 크게 바꾸므로, 변경 기록과 롤백 절차가 필요하다.

### 제로샷(Zero-shot) / 퓨샷(Few-shot)
**한 줄 정의**: 예시 없이 지시만 주는 방식이 제로샷, 몇 개의 예시를 같이 주는 방식이 퓨샷.

**핵심 이론**
- 퓨샷은 출력 형식/톤/판단 기준을 학습시키는 “미니 규격서” 역할을 한다.
- 다만 예시 자체가 민감정보/저작권 이슈가 되지 않게 관리해야 한다.

### 온도(Temperature) / Top-p
**한 줄 정의**: 생성 다양성을 조절하는 파라미터(높을수록 다양하지만 일관성이 줄 수 있음).

**핵심 이론**
- 정답이 정해진 업무(정책 요약, 숫자 표)는 낮은 다양성이 유리할 때가 많다.
- 아이데이션(슬로건, 이름 생성)은 다양성이 유리할 때가 많다.

**실무 포인트**
- “업무 유형별 기본값”을 정해두면 팀의 품질이 안정된다.

### 툴 호출(Tool/Function Calling)
**한 줄 정의**: 모델이 외부 시스템(검색, DB, 이메일 등)을 호출해 작업을 이어가는 패턴.

**핵심 이론**
- 툴 호출은 에이전트형의 핵심 기능이지만, 동시에 보안 사고의 주요 경로가 된다.
- OWASP는 플러그인/툴 설계 미흡(insecure plugin design)과 과도한 자율성(excessive agency)을 위험으로 본다.

**실무 포인트**
- 최소 권한, 승인 게이트, 감사 로그, 입력 검증이 필수다.

## 5) 평가 / 운영(LLMOps)

### 평가(Eval)
**한 줄 정의**: 모델/프롬프트/시스템의 품질을 “반복 측정 가능”하게 만드는 절차.

**핵심 이론**
- 1회 데모가 아니라, **대표 샘플 세트(골든 세트)**로 반복 측정해야 운영이 된다.
- 평가는 “정확도”만이 아니라 “유해 출력, 민감정보, 정책 위반, 비용/지연”까지 포함해야 한다.

**실무 포인트**
- 프롬프트/모델/검색 설정 변경은 반드시 회귀 테스트(Regression)를 거친다.

**학습 자료**
- NIST AI RMF 1.0: https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10
- NIST AI 600-1(Generative AI Profile, 2024-07-26): https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-generative-artificial-intelligence

### 모니터링(Monitoring) / 관측성(Observability)
**한 줄 정의**: 운영 중에 품질/비용/리스크 신호를 지속적으로 추적하는 것.

**핵심 이론**
- 모델은 시간이 지나며 드리프트가 생길 수 있다.
- 관측성이 없으면 “좋아졌는지/나빠졌는지”를 운영이 아닌 감각으로 판단하게 된다.

**실무 포인트**
- 최소라도 “오류 유형 TOP 3, 비용, 지연, 민감정보 입력 시도”는 매달 본다.

### LLMOps
**한 줄 정의**: LLM 기반 시스템을 배포·평가·모니터링·로그·비용 관점에서 운영하는 방법론/관행.

**핵심 이론**
- MLOps가 “모델 배포/재현성”에 강했다면, LLMOps는 “프롬프트/검색/툴 호출/정책”까지 운영 범위가 넓다.

**실무 포인트**
- 운영의 단위는 “모델”이 아니라 “업무 워크플로우(입력→출력→검토→승인)”다.

## 6) 보안 / 리스크(필수 확장)

### 프롬프트 인젝션(Prompt Injection)
**한 줄 정의**: 입력에 악성 지시를 섞어 규칙을 우회하거나 정보를 빼내려는 공격.

**핵심 이론**
- 직접 인젝션(사용자 입력)뿐 아니라, 문서/RAG 검색 결과에 숨겨진 지시로 일어나는 “간접 인젝션”도 문제가 된다.
- OWASP는 Prompt Injection을 최상위 위험으로 분류한다.

**실무 포인트**
- “근거 문서만 사용” 같은 문구만으로는 부족하다. 입력 검증, 툴 권한 통제, 출력 검토가 필요하다.

**학습 자료**
- OWASP LLM01 Prompt Injection: https://owasp.org/www-project-top-10-for-large-language-model-applications/

### Insecure Output Handling(불안전한 출력 처리)
**한 줄 정의**: LLM 출력 결과를 검증 없이 실행/렌더링/저장해 2차 피해가 발생하는 위험.

**핵심 이론**
- 출력이 코드/SQL/HTML/메일 발송 같은 “행동”으로 이어질 때, 검증이 없으면 사고가 커진다.
- OWASP는 이를 주요 위험(LLM02)으로 분류한다.

**실무 포인트**
- “출력은 제안일 뿐, 실행은 별도 검증” 원칙을 지키는 게 기본이다.

### 민감정보 노출(Sensitive Information Disclosure) / 데이터 유출(Exfiltration)
**한 줄 정의**: 민감정보가 모델 출력이나 로그, 연동 경로를 통해 노출/반출되는 사고.

**핵심 이론**
- 유출은 “입력”뿐 아니라 “출력/로그/권한/커넥터”에서도 터진다.
- OWASP는 민감정보 노출(LLM06)을 주요 위험으로 분류한다.

**실무 포인트**
- 로그에는 원문 대신 요약을 남기거나, 접근권한/보관기간을 엄격히 관리한다.

### 레드팀(Red Teaming)
**한 줄 정의**: 실제 공격/오남용을 가정해 취약점을 찾는 모의훈련/테스트.

**핵심 이론**
- 레드팀은 “보안팀의 행사”가 아니라, 운영 전 필수 품질 점검에 가깝다.
- 작은 테스트 질문 목록만 있어도 사고를 크게 줄일 수 있다.

### 모델 카드(Model Card)
**한 줄 정의**: 모델의 목적/사용 범위/평가/한계를 문서로 공개하는 형식.

**핵심 이론**
- 모델 카드가 있으면 “어디까지 믿어도 되는지”가 명확해져 과신(Overreliance)을 줄인다.
- OWASP도 과신(Overreliance)을 주요 위험으로 분류한다.

**학습 자료**
- Model Cards for Model Reporting: https://arxiv.org/abs/1810.03993

## 7) 표준 / 제도(운영 기준)

### EU AI Act
**한 줄 정의**: EU의 AI 규제 체계(위험 수준 기반).

**핵심 이론**
- 조직이 AI를 운영할 때 “누가/어떻게 안전하게 쓰는가”가 제도 요구사항과 연결된다.
- AI 리터러시(Article 4)는 “교육 권고”가 아니라 운영 책임의 일부로 해석될 여지가 있다.

**학습 자료**
- Regulation (EU) 2024/1689(원문): https://eur-lex.europa.eu/eli/reg/2024/1689/oj

### NIST AI RMF / NIST AI 600-1
**한 줄 정의**
- NIST AI RMF: AI 위험 관리 프레임워크(자발적 도입을 위한 기준)
- NIST AI 600-1: 생성형 AI에 특화된 프로파일(보완 가이드)

**학습 자료**
- NIST AI RMF 1.0: https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10
- NIST AI 600-1(2024-07-26): https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-generative-artificial-intelligence

### NIST IR 8596(Cyber AI Profile, 초안)
**한 줄 정의**: NIST CSF 2.0 관점에서 “AI 시대의 사이버보안”을 다루는 프로파일 초안.

**핵심 이론**
- AI는 “모델”만이 아니라 데이터/툴/파이프라인 전체가 공격면이 된다.
- 따라서 조직은 AI 도입과 동시에 보안 프레임워크를 재정렬해야 한다.

**학습 자료**
- NIST IR 8596 iPRD(2025-12-16): https://csrc.nist.gov/pubs/ir/8596/iprd

### ISO/IEC 42001 / 23894 / 42005
**한 줄 정의**
- ISO/IEC 42001: 조직 차원의 AI 경영 시스템(AIMS)
- ISO/IEC 23894: AI 리스크 관리 지침
- ISO/IEC 42005: AI 시스템 영향평가(Impact Assessment)

**실무 포인트**
- 표준을 “서류”로만 보면 실패한다. 최소한의 정책/인벤토리/승인 게이트/로그로 번역해서 현장에 붙여야 한다.

**학습 자료**
- ISO/IEC 42001: https://www.iso.org/standard/42001
- ISO/IEC 23894: https://www.iso.org/standard/77304.html
- ISO/IEC 42005: https://www.iso.org/standard/42005
