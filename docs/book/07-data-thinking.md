---
title: 07. 데이터 사고와 스프레드시트
---

## 1. 데이터 사고는 왜 필요한가
AI는 결국 데이터를 다루는 도구다. 문서와 회의를 잘 정리한다고 해도, 근거가 되는 데이터가 부정확하면 결과는 쉽게 흔들린다. 데이터 사고란 “숫자를 잘 다루는 기술”이 아니라 **무엇을 기록하고, 어떻게 해석할지 결정하는 능력**이다. 실무자에게 데이터 사고는 선택이 아니라 기본 소양이다. 좋은 데이터 사고가 있으면 AI 결과를 검증할 수 있고, 업무의 기준이 명확해진다. 결국 데이터 사고는 AI 시대의 “업무 기준을 세우는 사고”다.

현실에서 데이터 사고는 “의사결정의 사슬”을 이해하는 것에서 시작한다. 한 번의 숫자 입력이 보고서, 회의, 전략으로 이어진다. 예를 들어 월간 매출이 잘못 입력되면, 예산 배분이 흔들리고, 결국 실행 계획이 바뀐다. 데이터 사고는 이런 연결 구조를 보는 능력이다. 이 능력이 있으면 작은 실수를 줄일 수 있고, AI의 결과도 더 신뢰할 수 있다.

또한 데이터 사고는 “기록 습관”과 연결된다. 어떤 정보를 기록하고, 어떻게 정리하는지가 장기적으로 업무 성과를 좌우한다. 현업 중심 조직에서 데이터 사고는 거창한 분석이 아니라, 일상 기록을 **정확하고 일관되게** 남기는 습관이다. AI는 그 기록을 빠르게 분석해 주지만, 기록의 질은 사람이 만든다. 그래서 데이터 사고는 기술이 아니라 **업무 습관**이다.

> 핵심 요약: 데이터 사고는 “숫자를 잘 다루는 기술”이 아니라 **기준을 세우는 습관**이다.

빠른 첫 적용(예시)

- 핵심 지표 3개만 정해 기록하기
- 날짜/단위/기준 시점을 반드시 표기하기
- 원본 데이터와 요약 데이터를 분리해서 저장하기

## 2. 데이터와 정보의 차이를 이해하기
데이터는 사실의 조각이고, 정보는 의미를 가진 데이터다. 예를 들어 “100”은 데이터지만, “이번 달 매출이 100억”은 정보다. 데이터는 맥락이 없으면 의미가 없다. 맥락은 누가, 언제, 어떤 기준으로 기록했는지를 말한다. AI는 데이터를 정보로 바꾸는 데 도움을 주지만, 맥락을 결정하는 것은 사람이다. 데이터 사고는 **맥락을 붙이는 능력**이다.

| 구분 | 정의 | 예시 |
|---|---|---|
| 데이터 | 맥락 없는 사실 | 100, 2026-01-01 |
| 정보 | 의미가 있는 데이터 | 2026-01-01 매출 100억 |

맥락이 없는 데이터는 오해를 만든다. 예를 들어 “고객 수 100명”이라는 숫자가 있더라도, “신규 고객인지, 전체 고객인지”가 없다면 의미가 달라진다. 또한 기준 시점이 다르면 같은 숫자도 다른 의미가 된다. 데이터 사고는 숫자보다 “기준과 정의”를 먼저 확인하는 태도다. 실무자에게 이 태도는 AI 활용의 기본 안전장치가 된다.

데이터와 정보의 차이를 이해하면 문서와 회의의 질도 올라간다. 회의에서 숫자를 말할 때 “이 숫자가 무엇을 의미하는지”를 명확히 하면 논쟁이 줄어든다. 보고서에서도 “정의가 있는 숫자”는 신뢰를 만든다. 데이터 사고는 결국 **의사소통의 명확성**을 높인다.

## 3. 데이터 라이프사이클을 보라
데이터는 생성 → 저장 → 활용 → 폐기의 흐름을 가진다. 이 흐름을 이해하지 못하면 데이터는 쌓이기만 하고 사용되지 않는다. 생성 단계에서 기준이 없으면 저장 단계에서 이미 품질이 흔들린다. 활용 단계에서 오류를 발견해도 늦다. 폐기 기준이 없으면 데이터는 계속 쌓여 비용과 리스크가 커진다. 데이터 사고는 이 전체 흐름을 관리하는 관점이다. 즉 데이터는 한 번 만들고 끝나는 것이 아니라, **관리해야 하는 자산**이다.

| 단계 | 핵심 질문 | 대표 작업 |
|---|---|---|
| 생성 | 무엇을 기록할 것인가 | 입력 기준 정의 |
| 저장 | 어디에 둘 것인가 | 권한/보안 설정 |
| 활용 | 어떻게 쓸 것인가 | 분석/보고 |
| 폐기 | 언제 지울 것인가 | 보존/삭제 규칙 |

각 단계에는 책임이 필요하다. 생성 단계에서는 입력 기준을 정할 사람이 필요하고, 저장 단계에서는 접근 권한과 보안 기준을 관리할 사람이 필요하다. 활용 단계에서는 분석 기준을 정하고, 폐기 단계에서는 보존 기간과 삭제 규칙을 정해야 한다. 책임이 없으면 데이터는 방치되고, 결국 신뢰를 잃는다. 데이터 사고는 “누가 책임지는가”를 명확히 하는 일이다.

이 라이프사이클을 잘 이해하면 AI 활용도 쉬워진다. 예를 들어 활용 단계에서 AI를 쓰려면, 저장 단계에서 데이터가 정리되어 있어야 한다. 폐기 기준이 없으면 AI가 오래된 데이터를 섞어 결과를 왜곡할 수 있다. 데이터 사고는 기술보다 **흐름과 책임**을 보는 능력이다.

## 4. 스프레드시트는 가장 강력한 데이터 도구다
실무자에게 데이터는 대부분 스프레드시트로 시작한다. 매출표, 고객 목록, 재고 현황, 인력 데이터가 모두 스프레드시트에 있다. 이 도구는 접근성이 높고 빠르지만, 오류가 쉽게 들어간다. 셀 하나가 바뀌면 전체 결과가 틀어질 수 있다. 또한 파일 복사로 인해 버전 관리가 어렵다. 그래서 스프레드시트는 “편리함과 위험”을 동시에 가진다. 데이터 사고는 이 도구의 한계를 이해하고 보완하는 데서 시작한다.

스프레드시트의 가장 큰 위험은 “누가 무엇을 바꿨는지” 추적하기 어렵다는 점이다. 파일이 여러 개로 복사되면 어떤 것이 최신인지 알 수 없다. 이 문제는 작은 팀에서도 자주 발생한다. 그래서 스프레드시트는 최소한의 규칙이 필요하다. 예를 들어 “원본 파일은 수정 금지, 사본에서 작업” 같은 규칙만 있어도 오류를 줄일 수 있다.

| 위험 | 발생 원인 | 예방 규칙 |
|---|---|---|
| 버전 혼선 | 파일 복사 | 원본 1개 유지 |
| 입력 오류 | 자유 입력 | 드롭다운/형식 제한 |
| 중복 데이터 | 병합 실패 | 중복 체크 규칙 |

또한 스프레드시트는 구조화되지 않은 데이터를 다루기 쉬운 장점이 있다. 하지만 이 자유로움이 기준을 무너뜨릴 수 있다. 열 제목이 바뀌거나, 데이터 형식이 뒤섞이면 분석이 어려워진다. 따라서 스프레드시트는 **표준 열 이름, 데이터 형식, 입력 규칙**이 필요하다. 데이터 사고는 이 작은 규칙을 반복하는 습관에서 시작한다.

## 5. 데이터 품질 6요소
데이터 품질은 여섯 가지로 정리할 수 있다. **정확성, 완전성, 일관성, 적시성, 유효성, 유일성**이다. 정확성은 사실과 맞는지, 완전성은 빠짐이 없는지, 일관성은 기준이 통일되는지, 적시성은 최신인지, 유효성은 규칙을 지키는지, 유일성은 중복이 없는지를 의미한다. 이 여섯 요소는 데이터 품질의 기본 기준이다. 현업 중심 조직에서도 이 기준만 이해하면 데이터 품질을 관리할 수 있다.

| 품질 요소 | 의미 | 빠른 확인 |
|---|---|---|
| 정확성 | 사실과 일치 | 원문 대조 |
| 완전성 | 누락 없음 | 빈칸 비율 |
| 일관성 | 기준 통일 | 용어/단위 통일 |
| 적시성 | 최신성 | 업데이트 날짜 |
| 유효성 | 규칙 준수 | 형식/범위 체크 |
| 유일성 | 중복 없음 | 중복 행 탐지 |

각 요소는 실무에서 바로 확인할 수 있다. 정확성은 “숫자가 원문과 맞는가”, 완전성은 “빈칸이 얼마나 있는가”, 일관성은 “같은 의미가 같은 표현을 쓰는가”로 확인된다. 적시성은 “업데이트 날짜가 최근인가”, 유효성은 “형식 규칙을 지키는가”, 유일성은 “중복이 있는가”로 확인된다. 이 기준을 체크리스트로 만들면 품질 관리가 쉬워진다.

품질 요소는 서로 연결되어 있다. 예를 들어 일관성이 낮으면 정확성도 떨어진다. 같은 데이터를 다른 방식으로 기록하면 숫자가 달라지기 때문이다. 또한 완전성이 낮으면 분석 결과가 왜곡된다. 품질 요소는 하나만 관리하는 것이 아니라 **균형**을 보아야 한다. 데이터 사고는 이 균형을 판단하는 능력이다.

실무에서는 품질 기준을 “최소 수준”으로 정하는 것이 중요하다. 모든 데이터를 완벽하게 관리하려고 하면 현업이 부담을 느낀다. 예를 들어 “핵심 항목 5개는 반드시 채운다” 같은 최소 기준을 정하면 지속적으로 관리할 수 있다. 또한 품질 기준은 자동화할 수 있다. 드롭다운, 입력 제한, 중복 체크 같은 간단한 기능만으로도 품질이 크게 좋아진다. 데이터 품질은 복잡한 기술이 아니라 **작은 규칙의 반복**으로 개선된다.

## 6. 품질 문제의 비용
데이터 품질 문제는 보이지 않는 비용을 만든다. 잘못된 숫자는 잘못된 결정을 만들고, 잘못된 결정을 다시 수정하는 데 시간이 들어간다. 예를 들어 고객 수가 중복되어 있으면 마케팅 비용이 낭비된다. 재고 데이터가 틀리면 판매 계획이 어긋난다. 데이터 오류는 단순한 실수가 아니라 **조직 비용**이다. AI는 품질이 낮은 데이터를 빠르게 처리할 수 있지만, 결국 잘못된 결과를 더 빠르게 만든다. 그래서 데이터 품질은 기술 문제가 아니라 **경영 문제**다.

품질 문제는 신뢰 문제로 이어진다. 데이터가 자주 틀리면 사람들은 데이터를 믿지 않는다. 결국 중요한 결정에서 데이터가 배제된다. 이 상태가 지속되면 데이터는 존재하지만 쓰이지 않는 자산이 된다. AI를 도입해도 효과가 낮다. 데이터 사고는 “데이터를 믿을 수 있게 만드는 과정”이다. 신뢰가 있어야 AI도 신뢰를 얻는다.

또한 품질 문제는 재작업 비용을 만든다. 보고서를 다시 만들거나, 회의를 다시 해야 하는 상황이 생긴다. 이 비용은 숫자로 보이지 않지만 조직에는 큰 부담이다. 데이터 사고는 이런 숨은 비용을 줄이는 전략이다. 데이터 품질은 결국 **업무 효율**과 직결된다.

품질 문제의 경고 신호

- 같은 보고서가 여러 버전으로 존재
- 회의에서 숫자 논쟁이 반복
- 기준 시점이 문서마다 다름

신뢰가 무너지면 가장 먼저 나타나는 증상은 “각자 엑셀을 따로 만든다”는 현상이다. 같은 보고서를 여러 사람이 각기 다른 버전으로 만들고, 회의에서 숫자가 맞지 않아 논쟁이 생긴다. 이때 AI를 도입해도 혼란은 줄지 않는다. 데이터 사고는 이런 상황을 예방하는 방법이다. 결국 데이터 품질은 “한 번의 정확한 기록”에서 시작된다.

## 7. 입력 기준과 표준화가 핵심이다
데이터 품질을 높이는 가장 현실적인 방법은 입력 기준을 만드는 것이다. 예를 들어 지역은 “서울, 부산, 대구”처럼 표준 목록으로 입력하도록 하고, 날짜는 “YYYY-MM-DD”로 통일한다. 이렇게 하면 오류가 크게 줄어든다. 스프레드시트에서도 드롭다운 목록을 쓰면 표준화가 쉽다. 입력 기준은 작은 규칙이지만, 품질을 크게 높인다. 데이터 사고는 결국 **작은 기준을 반복하는 습관**이다.

입력 기준은 “현업이 지키기 쉬운 수준”이어야 한다. 너무 복잡하면 지켜지지 않는다. 예를 들어 코드 체계를 너무 세분화하면 입력이 느려지고 오류가 늘어난다. 따라서 입력 기준은 단순하고 명확해야 한다. 작은 규칙이라도 모든 사람이 지키면 효과는 크다. 데이터 사고는 완벽함보다 **지속 가능성**을 선택하는 일이다.

| 입력 기준 예시 | 효과 |
|---|---|
| 날짜: YYYY-MM-DD | 기준 시점 혼선 감소 |
| 지역: 표준 목록 | 표현 불일치 감소 |
| 금액: 억 단위 | 단위 혼선 감소 |

입력 기준을 만들 때는 “예외 처리”도 포함해야 한다. 예외가 많으면 기준이 무너진다. 따라서 예외를 줄이고, 꼭 필요한 예외는 별도로 표시하도록 규칙을 만든다. 예외를 명확히 하면 데이터 해석이 쉬워진다. 입력 기준은 단순한 규칙이 아니라 **데이터의 의미를 지키는 장치**다.

## 8. 메타데이터와 데이터 사전
메타데이터는 “데이터에 대한 데이터”다. 데이터가 언제 생성되었는지, 누가 관리하는지, 어떤 의미인지 설명한다. 메타데이터가 없으면 데이터는 숫자의 집합에 불과하다. 예를 들어 “매출”이 총매출인지 순매출인지 정의가 없으면 보고서가 달라진다. 이 문제를 해결하는 것이 데이터 사전이다. 데이터 사전은 조직 내 용어와 정의를 정리한 문서다. 현업 중심 조직이라도 작은 데이터 사전만으로 혼란을 크게 줄일 수 있다.

메타데이터는 설명 책임을 만든다. 누가 이 데이터를 만들었고, 어떤 기준을 썼는지를 알 수 있어야 한다. 그래야 데이터가 신뢰를 얻는다. 특히 AI를 사용할 때는 메타데이터가 더욱 중요하다. AI는 맥락을 모른 채 데이터를 처리하기 때문이다. 메타데이터는 AI에게 **맥락을 제공하는 도구**다.

데이터 사전은 처음부터 완벽할 필요가 없다. 핵심 용어 10개만 정리해도 효과가 크다. 예를 들어 “매출, 비용, 고객” 같은 용어의 정의를 정리하면, 보고서의 일관성이 높아진다. 작은 사전이 쌓이면 큰 자산이 된다. 데이터 사고는 작은 정의를 쌓아가는 과정이다.

간단 데이터 사전 템플릿

| 용어 | 정의 | 담당 부서 |
|---|---|---|
| 매출 | 부가세 제외 금액 | 재무 |
| 고객 | 1회 이상 구매 | 영업 |

데이터 사전을 유지하려면 “관리 책임자”가 필요하다. 누가 정의를 업데이트하고 승인할지 정해야 한다. 예를 들어 새로운 용어가 생기면, 담당자가 정의를 추가하고 조직에 공지한다. 이 절차가 없으면 사전은 금방 낡는다. 데이터 사전은 문서가 아니라 **업무 기준의 저장소**다. 이 저장소가 유지될 때 데이터 사고가 지속된다.

## 9. 데이터 정리와 결측치 처리
데이터는 정리가 필요하다. 오탈자 수정, 중복 제거, 형식 통일이 기본이다. 이 작업은 단순한 청소가 아니라 기준을 만드는 과정이다. 결측치(비어 있는 값)는 특히 중요하다. 결측치를 무시하면 분석 결과가 왜곡될 수 있다. 단순히 평균값으로 채우는 것은 의미를 바꿀 위험이 있다. 현업 중심 조직에서는 결측치를 줄이는 입력 규칙이 가장 현실적인 해결책이다. 데이터 정리는 “이 데이터를 믿을 수 있는가”라는 질문에 답하는 과정이다.

데이터 정리는 “기록”을 남겨야 한다. 어떤 값을 어떻게 바꿨는지 기록하지 않으면 나중에 원인을 찾을 수 없다. 작은 조직이라도 “정리 로그”를 남기면 품질이 크게 개선된다. 예를 들어 “서울시 → 서울”로 통일했다는 기록만 있어도 분석 결과를 설명할 수 있다. 정리는 결과만큼 **과정 기록**이 중요하다.

결측치 처리 기본 원칙

- 무시하지 않고 원인을 표시
- 평균으로 채울 땐 근거 기록
- 중요한 필드는 입력 단계에서 막기

결측치 처리는 업무 맥락에 따라 달라진다. 예를 들어 고객 연락처가 비어 있으면 연락 불가능이라는 의미일 수 있다. 반대로 매출 데이터가 비어 있으면 단순 누락일 수 있다. 이 차이를 구분하지 않으면 분석이 왜곡된다. 그래서 결측치 처리에는 “사유 구분”이 필요하다. 데이터 사고는 숫자뿐 아니라 **빈칸의 의미**까지 읽는 능력이다.

## 10. 데이터 거버넌스와 역할
데이터는 책임자가 있어야 관리된다. 데이터 오너(Owner)는 정의와 기준을 결정하고, 데이터 스튜어드(Steward)는 실무에서 품질을 관리한다. 이 역할이 없으면 데이터는 부서마다 다르게 관리된다. AI를 도입하면 이 문제는 더 커진다. AI는 데이터를 소비하지만 책임지지 않는다. 책임은 조직의 사람에게 있다. 데이터 거버넌스는 복잡한 제도가 아니라 **역할과 기준을 명확히 하는 것**이다.

| 역할 | 책임 |
|---|---|
| 오너 | 정의/정책 결정 |
| 스튜어드 | 품질/표준 관리 |
| 사용자 | 입력/검증 |

작은 조직에서도 역할 분리는 필요하다. 예를 들어 한 사람이 오너와 스튜어드 역할을 동시에 맡을 수는 있지만, “누가 책임지는가”는 명확해야 한다. 이 책임이 없으면 데이터 오류가 반복된다. 데이터 거버넌스는 회의와 문서처럼 **책임 구조**에서 시작한다.

거버넌스는 규칙을 강요하는 것이 아니라, 기준을 공유하는 것이다. “이 데이터는 이렇게 쓰인다”는 합의가 있을 때, AI도 안정적으로 작동한다. 합의 없이 도구만 도입하면, 데이터는 혼란이 되고 AI는 신뢰를 잃는다. 데이터 거버넌스는 AI 도입의 기반이다.

거버넌스는 작은 회의체로도 충분하다. 예를 들어 월 1회 데이터 기준을 점검하는 간단한 회의를 운영하면 기준이 유지된다. 이 회의에서 정의 변경, 품질 이슈, 표준화 범위를 논의하면 된다. 거버넌스는 거창한 위원회가 아니라 **정기적인 기준 점검 습관**이다. 작은 조직일수록 이런 습관이 효과적이다.

## 11. 정책: 분류·보존·폐기
데이터는 민감도에 따라 분류되어야 한다. 공개 데이터, 내부 데이터, 기밀 데이터는 처리 방식이 달라야 한다. 또한 보존과 폐기 기준이 필요하다. 오래된 데이터는 의사결정을 왜곡할 수 있고, 보관 비용을 늘린다. 기록 관리 기준(예: ISO 15489)은 이런 원칙을 제시한다. AI가 데이터를 많이 소비할수록 보존 기준은 더 중요해진다. 정책이 없으면 데이터는 자산이 아니라 부담이 된다.

| 분류 | 예시 | 처리 기준 |
|---|---|---|
| 공개 | 홍보 자료 | 제한 없음 |
| 내부 | 운영 문서 | 내부 사용자 |
| 기밀 | 개인정보 | 입력 금지/제한 |

보존 정책에는 “최소 기간과 최대 기간”이 필요하다. 너무 빨리 삭제하면 추적이 어렵고, 너무 오래 보관하면 보안 위험이 커진다. 예를 들어 정책 문서는 장기 보관이 필요하지만, 일상 메모는 짧은 보관으로 충분하다. AI가 만든 문서도 동일한 기준을 적용해야 한다. 자동 생성이라고 해서 기록이 아닌 것은 아니다.

폐기 정책은 보안과 직결된다. 오래된 개인정보나 계약 정보가 남아 있으면 위험이 커진다. 따라서 폐기는 “삭제의 책임”을 포함한다. 누가 삭제를 승인하고, 어떻게 삭제되는지 정해야 한다. 데이터 사고는 보관뿐 아니라 **폐기의 책임**까지 포함한다.

분류 정책은 실무자가 쉽게 적용할 수 있어야 한다. 예를 들어 “고객 식별 정보는 기밀”, “내부 운영 자료는 내부”, “외부 공개 문서는 공개”처럼 단순한 분류 기준이 필요하다. 이 기준을 교육하지 않으면 분류는 형식적으로만 남는다. 분류 정책은 **현장 적용 가능성**이 가장 중요하다. 실무자가 이해하고 지킬 수 있는 기준이어야 한다.

보존과 폐기는 법무·감사와도 연결된다. 예를 들어 계약 관련 문서는 법적 분쟁에 대비해 장기간 보관해야 할 수 있다. 반면 일상적인 메모는 장기간 보관할 이유가 없다. 이런 차이를 반영한 “보존 스케줄”을 만들어야 한다. 스케줄이 없으면 보관이 과도해지고, 보안 위험이 커진다. 데이터 사고는 보존과 폐기를 **업무 목적에 맞게 설계하는 능력**이다.

## 12. 개인정보와 윤리
데이터에는 개인정보가 포함될 수 있다. 개인정보는 법적 보호 대상이고, 잘못 다루면 신뢰가 무너진다. AI가 개인정보를 학습하거나 노출하면 심각한 문제가 발생한다. 그래서 개인정보는 최소화하거나 익명화해야 한다. 또한 데이터 편향은 윤리 문제로 이어질 수 있다. 특정 집단이 과도하게 반영된 데이터는 차별적 결과를 만들 수 있다. 데이터 사고는 기술이 아니라 **윤리와 책임의 문제**다.

개인정보 최소화 체크

- 목적에 꼭 필요한가?
- 익명화/가명처리가 가능한가?
- 외부 AI 입력을 피할 수 있는가?

개인정보를 다룰 때는 “필요한 것만” 사용하는 원칙이 중요하다. 업무 목적에 필요하지 않은 개인정보는 수집하지 않는 것이 가장 안전하다. 또한 데이터 마스킹이나 가명처리를 통해 민감 정보를 줄일 수 있다. AI를 사용할 때는 이런 보호 조치를 먼저 해야 한다. 개인정보 보호는 규정 준수 이상의 문제다. 조직의 신뢰를 지키는 기본이다.

편향 문제는 데이터 다양성과 연결된다. 예를 들어 특정 지역의 데이터만 있으면 전국을 대표할 수 없다. 특정 고객군만 반영되면 결과가 왜곡된다. AI는 이런 편향을 스스로 판단하지 못한다. 따라서 데이터 사고는 “이 데이터가 누구를 대표하는가”를 질문하는 능력이다. 윤리는 기술이 아니라 **대표성과 공정성**의 문제다.

윤리적 관점에서는 “투명성”도 중요하다. 데이터가 어떻게 수집되었고, 어떤 기준으로 분석되었는지 설명할 수 있어야 한다. 설명할 수 없는 분석은 신뢰를 얻기 어렵다. 따라서 데이터 사용 과정은 가능하면 문서로 남겨야 한다. 이것이 AI 결과에 대한 책임을 명확히 한다. 윤리는 선택이 아니라 **신뢰의 조건**이다.

## 13. 이론과 모형: DIKW와 CRISP-DM
데이터 사고를 이해하는 데 도움이 되는 두 가지 모델이 있다. 첫째는 DIKW(데이터-정보-지식-지혜) 모델이다. AI는 데이터와 정보를 잘 다루지만, 지혜는 사람의 판단 영역이다. 둘째는 CRISP-DM이다. 비즈니스 이해 → 데이터 이해 → 데이터 준비 → 모델링 → 평가 → 배포의 흐름이다. 이 모델은 “데이터 준비가 가장 많은 시간을 차지한다”는 현실을 보여준다. 두 모델은 실무자에게 AI의 역할을 이해시키는 좋은 기준이다.

| 모델 | 핵심 메시지 |
|---|---|
| DIKW | AI는 지혜를 대신하지 못함 |
| CRISP-DM | 데이터 준비가 가장 오래 걸림 |

DIKW는 역할 분담을 명확히 하는 데도 유용하다. 데이터 단계는 기록 담당자가, 정보 단계는 보고서 작성자가, 지식 단계는 분석 담당자가 맡는다. 지혜 단계는 최종 의사결정자가 책임진다. AI는 데이터와 정보 단계에서 큰 도움을 주지만, 지혜를 대신할 수 없다. 이 구분을 이해하면 AI 활용의 범위가 명확해진다.

CRISP-DM은 “프로젝트 흐름”을 보여준다. 특히 데이터 준비 단계가 가장 많은 시간을 차지한다는 점은 실무자에게 중요한 메시지다. AI가 발전해도 데이터 준비는 여전히 사람의 몫이 많다. 따라서 AI 도입 전에 데이터 준비 시간을 예산에 포함해야 한다. 이 모델은 AI 프로젝트가 생각보다 **데이터 중심**임을 보여준다.

## 14. 데이터 품질 지표와 측정
데이터 품질은 지표로 관리해야 한다. 결측률, 오류율, 중복률 같은 지표만으로도 품질을 파악할 수 있다. 또한 문서 재작업률이나 보고서 수정 횟수도 데이터 품질을 보여준다. 지표는 숫자 자체가 아니라 개선의 방향을 보여준다. AI 도입 전후를 비교하면 효과를 확인할 수 있다. 지표는 **데이터 품질 개선의 나침반**이다.

| 지표 | 의미 |
|---|---|
| 결측률 | 빈칸 비율 |
| 오류율 | 잘못된 값 비율 |
| 중복률 | 중복 행 비율 |

지표를 사용할 때는 기준선이 필요하다. 도입 전의 평균 결측률이나 오류율을 기록해야 도입 후의 변화를 설명할 수 있다. 기준선이 없으면 성과를 체감하기 어렵다. 또한 지표를 너무 많이 만들면 관리가 어려워진다. 핵심 지표 3~5개만 유지하는 것이 현실적이다.

지표는 사람의 행동을 바꾸는 역할도 한다. 예를 들어 중복률을 측정하면 중복 입력을 줄이려는 노력이 생긴다. 재작업률을 측정하면 입력 단계의 기준이 강화된다. 데이터 사고는 지표를 통해 **조직의 습관**을 바꾸는 과정이다.

지표는 “정기 리뷰”와 함께 운영해야 한다. 예를 들어 월간 데이터 품질 리뷰를 진행하면, 작은 문제를 빠르게 발견할 수 있다. 지표를 기록만 하고 리뷰하지 않으면 변화가 없다. 또한 지표 결과를 현업과 공유하면 품질 개선이 더 빨라진다. 데이터 사고는 숫자 자체보다 **지속적인 개선 루프**를 만드는 데 있다.

## 15. 기본 분석: 집계, 비교, 추세
실무자에게 가장 중요한 분석은 복잡한 모델이 아니다. 집계, 비교, 추세 분석이 핵심이다. 예를 들어 월별 매출 추세를 보는 것만으로도 중요한 인사이트를 얻을 수 있다. AI는 이런 기본 분석을 빠르게 만들어준다. 하지만 기준이 다르면 비교가 의미가 없다. 따라서 분석 전에는 기준을 먼저 정해야 한다. 데이터 사고는 “비교 가능한 기준”을 만드는 능력이다.

| 분석 유형 | 목적 |
|---|---|
| 집계 | 전체 규모 파악 |
| 비교 | 전월/전년 차이 |
| 추세 | 방향성 확인 |

기본 분석에서 가장 흔한 실수는 “기준 혼합”이다. 예를 들어 어떤 달은 환불을 포함하고, 다른 달은 환불을 제외하면 비교가 틀린다. 그래서 분석 전에 기준을 명확히 문서화해야 한다. AI는 계산을 빠르게 해주지만, 기준을 정하지는 못한다. 기준을 정하는 일이 실무자의 핵심 역할이다.

또한 분석은 “목적”이 있어야 한다. 단순히 숫자를 보는 것은 의미가 없다. 예를 들어 “매출이 줄었는지 확인”, “고객 불만이 증가했는지 확인” 같은 목적이 있을 때 분석이 가치가 있다. 데이터 사고는 숫자보다 **질문을 먼저 정하는 능력**이다.

스프레드시트에서 피벗 테이블과 필터는 기본 분석의 핵심 도구다. 피벗은 데이터를 다양한 관점으로 요약할 수 있고, 필터는 필요한 정보만 빠르게 볼 수 있다. AI가 피벗을 자동으로 만들어줄 수 있어도, 기준을 선택하는 것은 사람이다. 어떤 기준으로 요약할지 결정하는 것이 분석의 시작이다. 도구는 계산을 돕지만, **질문의 방향**은 사람이 정한다.

또한 평균값만 보는 습관은 위험하다. 평균은 극단값에 영향을 받기 쉽고, 실제 상황을 왜곡할 수 있다. 예를 들어 평균 매출이 안정적이더라도 특정 고객군의 매출이 급감할 수 있다. 이때는 분포와 구간을 함께 봐야 한다. AI는 평균을 쉽게 계산해주지만, 무엇을 확인해야 할지는 사람이 결정해야 한다. 데이터 사고는 **숫자 뒤의 구조**를 보는 능력이다.

## 16. 시각화와 스토리텔링
데이터는 시각화될 때 이해가 쉬워진다. 막대 그래프는 비교에 강하고, 선 그래프는 추세에 강하다. 하지만 가장 중요한 것은 “메시지에 맞는 그래프를 고르는 것”이다. 또한 데이터 스토리텔링은 숫자를 이야기로 바꾸는 과정이다. AI는 문장을 만들어주지만, 어떤 메시지를 전달할지는 사람이 결정해야 한다. 데이터 사고는 **숫자를 설득으로 바꾸는 능력**이다.

| 목적 | 추천 그래프 |
|---|---|
| 비교 | 막대 그래프 |
| 추세 | 선 그래프 |
| 비율 | 원형/누적 막대 |

좋은 시각화는 “한 눈에 핵심을 보이게” 한다. 너무 많은 그래프는 오히려 혼란을 만든다. 따라서 가장 중요한 숫자 하나를 강조하는 것이 좋다. 예를 들어 “매출 감소”가 핵심이라면 그 추세를 단순한 선 그래프로 보여주는 것이 효과적이다. 시각화는 복잡함이 아니라 **명확성**을 목표로 해야 한다.

스토리텔링은 구조가 필요하다. 예를 들어 “문제 → 원인 → 대안 → 기대 효과”의 순서를 쓰면 설득력이 높아진다. AI는 이 구조를 채워줄 수 있지만, 문제와 대안을 선택하는 것은 사람의 몫이다. 데이터 사고는 결국 **의사결정 스토리**를 만드는 능력이다.

스토리텔링은 대상에 따라 달라져야 한다. 경영진에게는 짧고 결론 중심의 이야기, 실무자에게는 근거와 실행 방식이 강조된 이야기가 필요하다. 같은 데이터라도 전달 방식이 다르면 효과가 달라진다. AI는 문장을 바꿀 수 있지만, 어떤 방식이 적절한지는 사람이 결정해야 한다. 데이터 사고는 **대상 맞춤형 메시지 설계**까지 포함한다.

시각화와 스토리텔링의 균형도 중요하다. 대시보드만 제공하면 “왜 중요한지”가 전달되지 않을 수 있다. 반대로 이야기만 있고 그래프가 없으면 신뢰가 떨어질 수 있다. 따라서 핵심 그래프와 핵심 메시지를 함께 제공해야 한다. AI는 그래프 설명을 요약할 수 있지만, 어떤 메시지를 강조할지는 사람이 정해야 한다. 데이터 사고는 **그래프와 메시지의 균형**을 설계하는 능력이다.

## 17. AI와 데이터의 관계
AI는 데이터를 소비하고, 데이터는 AI의 품질을 결정한다. 좋은 데이터가 없으면 AI 결과도 불안정하다. 따라서 AI 도입 전에 데이터 준비도가 중요하다. 데이터가 어디에 있고, 얼마나 최신이며, 누가 관리하는지 파악해야 한다. 이 준비도가 낮으면 AI 도입은 실패할 가능성이 높다. AI는 데이터 관리의 결과다. 데이터 사고는 AI 성공의 조건이다.

AI 준비도 체크리스트

- 데이터 위치/소유자 파악
- 최신성 기준 존재
- 중복/결측 상태 확인
- 표준 용어 정의 존재

데이터 준비도는 간단한 질문으로 점검할 수 있다. “데이터가 어디에 있는가”, “누가 책임지는가”, “최근 업데이트는 언제인가”, “중복이나 결측이 얼마나 있는가.” 이 질문에 답할 수 없다면 AI 도입은 위험하다. 준비도가 낮은 상태에서 AI를 쓰면 결과는 일관되지 않고, 신뢰를 잃는다. AI는 기술이지만, 성패는 **데이터 준비 수준**에 달려 있다.

AI는 데이터 품질을 높이지 않는다. 오히려 낮은 품질을 빠르게 증폭시킬 수 있다. 그래서 AI 도입 전 “데이터 정리 프로젝트”가 필요할 수 있다. 이 프로젝트는 화려하지 않지만, AI 성공의 핵심이다. 데이터 사고는 기술을 쓰기 전에 **기초를 다지는 과정**이다.

또한 데이터 준비는 “연결 구조”를 정리하는 일이다. 데이터가 여러 파일과 시스템에 흩어져 있으면 AI가 정확한 답을 만들기 어렵다. 예를 들어 고객 데이터가 영업과 마케팅에서 서로 다르게 관리되면, AI는 고객 수를 잘못 계산할 수 있다. 따라서 데이터 준비에는 **단일 기준 데이터**(마스터 데이터)를 만드는 작업이 필요하다. 마스터 데이터가 있어야 AI도 일관된 결과를 낸다.

## 18. RAG를 위한 데이터 준비
RAG(검색 기반 생성)는 내부 문서를 검색해 답을 만드는 방식이다. 이 방식은 문서가 잘 정리되어 있을 때 효과가 크다. 문서가 흩어져 있거나 중복이 많으면 검색 결과가 불안정해진다. 따라서 RAG를 쓰려면 문서를 정리하고, 메타데이터를 붙이고, 업데이트 기준을 정해야 한다. RAG는 기술보다 **문서 정리와 데이터 관리**가 먼저다.

RAG를 위한 실무 준비는 “문서 구조화”다. 예를 들어 문서를 주제별로 분류하고, 작성일과 작성 부서를 메타데이터로 붙이는 것만으로도 검색 품질이 올라간다. 또한 문서를 적절한 크기로 나누어 저장하면 검색이 정확해진다. 이 과정이 없으면 RAG는 잘 작동하지 않는다. 즉 RAG는 기술보다 **문서 정리 작업**이 더 중요하다.

RAG 준비 체크리스트

- 문서 분류/태그 정리
- 작성일/부서 메타데이터
- 최신 문서 우선 규칙

또한 RAG에는 “최신성 관리”가 필요하다. 오래된 문서가 최신 정책처럼 답변에 포함되면 위험하다. 따라서 문서의 업데이트 날짜를 명확히 하고, 최신 문서를 우선적으로 사용하도록 기준을 정해야 한다. RAG는 최신성 관리가 없으면 오히려 위험해질 수 있다. 데이터 사고는 RAG의 안전장치다.

RAG를 운영할 때는 “검증 루프”가 필요하다. 예를 들어 AI가 답변한 내용이 실제 정책 문서와 일치하는지 확인하는 절차가 있어야 한다. 또한 잘못된 답변이 발견되면 문서를 수정하거나 메타데이터를 보완해야 한다. 이 루프가 없으면 RAG는 점점 불신을 받게 된다. RAG는 한 번 구축하고 끝나는 것이 아니라 **지속적으로 관리해야 하는 시스템**이다.

실무에서는 “테스트 질문 세트”를 만드는 것이 유용하다. 자주 묻는 질문 20~30개를 정리하고, RAG가 얼마나 정확히 답하는지 확인한다. 이 세트는 품질 검증의 기준이 된다. 또한 문서 업데이트가 있을 때마다 테스트를 반복하면 품질 변화를 빨리 발견할 수 있다. RAG는 데이터와 문서가 바뀌면 결과도 바뀌기 때문에, **지속적인 품질 검증**이 필수다.

## 19. 업종별 데이터 사고 포인트
업종마다 데이터의 성격이 다르다. 제조업은 설비·품질 데이터가 핵심이고, 금융업은 거래·리스크 데이터가 핵심이다. 헬스케어는 민감 정보와 설명 책임이 중요하고, 공공 분야는 투명성과 기록 관리가 중요하다. 유통업은 최신성과 속도가 중요하다. 같은 데이터라도 업종별 기준이 다르기 때문에, 데이터 사고는 업종 맥락을 반영해야 한다. 업종을 무시한 데이터 분석은 위험하다.

| 업종 | 핵심 데이터 | 주요 리스크 |
|---|---|---|
| 제조 | 설비/품질 | 안전/정확성 |
| 금융 | 거래/리스크 | 규제 위반 |
| 헬스케어 | 환자 정보 | 개인정보 |
| 공공 | 기록/정책 | 신뢰/감사 |
| 유통 | 재고/판매 | 최신성 |

제조업은 설비 데이터의 정확성이 곧 비용이다. 설비 점검 기록이 틀리면 예방 정비가 실패하고, 생산 차질이 발생한다. 금융업은 숫자 오류가 규제 위반으로 이어질 수 있다. 헬스케어는 환자 데이터의 오류가 직접적인 피해로 이어질 수 있다. 공공 분야는 기록이 곧 신뢰이기 때문에, 데이터 오류가 신뢰 붕괴로 이어진다. 업종별 데이터 사고는 단순한 분석이 아니라 **업종 리스크 관리**다.

유통·서비스 분야는 데이터의 최신성이 중요하다. 할인 정책, 재고 수준, 배송 기준은 자주 바뀐다. 이때 오래된 데이터를 섞으면 고객 불만이 생긴다. 따라서 유통업에서는 기준 시점을 명확히 해야 한다. 업종별 기준은 결국 “어떤 리스크가 가장 큰가”를 기준으로 정해야 한다. 데이터 사고는 업종 리스크를 줄이는 전략이다.

인사나 교육 분야에서도 데이터 사고는 중요하다. 예를 들어 직원 만족도 조사를 분석할 때, 응답률이 낮으면 결과를 과대해석할 위험이 있다. 또한 채용 데이터는 편향 문제와 연결된다. 이런 영역에서는 데이터의 윤리성이 특히 중요하다. 영업 분야에서는 매출 데이터의 정의가 중요하고, 마케팅 분야에서는 캠페인 효과 측정 기준이 중요하다. 업종뿐 아니라 **부서별 맥락**도 데이터 사고에 포함해야 한다.

고객 지원(콜센터)과 같은 서비스 조직에서는 문의 유형 데이터가 핵심이다. 문의가 늘었는지, 특정 유형이 급증했는지를 빠르게 파악해야 한다. 이때 문의 분류 기준이 바뀌면 비교가 어렵다. 따라서 분류 기준을 고정하고, 변경 시에는 명확히 기록해야 한다. 또한 고객 불만 데이터는 감정이 포함될 수 있으므로, 숫자만으로 판단하면 오해가 생긴다. 이런 영역에서는 데이터 사고가 “정확한 분류 + 맥락 해석”을 동시에 요구한다.

## 20. 도입 순서와 조직 문화
데이터 사고는 한 번에 바뀌지 않는다. 작은 영역부터 시작해야 한다. 예를 들어 한 부서의 보고서 데이터 기준을 통일하는 것부터 시작한다. 작은 성공이 쌓이면 조직 전체로 확산할 수 있다. 이 과정에서 교육과 커뮤니케이션이 필요하다. 데이터 사고는 기술보다 문화의 문제다. 조직 문화가 바뀌어야 데이터가 자산이 된다.

확산 단계(요약)

1) 작은 기준 합의
2) 파일럿 적용
3) 성과 공유
4) 전사 확산

작은 성공은 “눈에 보이는 효과”를 만든다. 예를 들어 보고서 수정 횟수가 줄거나, 회의 시간이 단축되는 것처럼 체감할 수 있는 효과가 있어야 한다. 이런 효과가 있어야 조직이 데이터 사고의 필요성을 인정한다. 데이터 사고는 강요로 되지 않는다. 체감 가능한 성과가 문화를 바꾼다.

조직 내 “데이터 챔피언”이 필요하다. 이 사람은 데이터 기준을 지키고, 좋은 사례를 공유하는 역할을 한다. 챔피언이 없으면 데이터 사고는 흐지부지된다. 데이터 사고는 결국 사람의 습관을 바꾸는 일이기 때문이다. AI 도입도 마찬가지로 **내부 리더십**이 중요하다.

데이터 문화는 인센티브와도 연결된다. 예를 들어 데이터 품질을 개선한 팀을 인정하거나, 오류를 줄인 사례를 공유하면 동기 부여가 된다. 반대로 데이터 기준을 지키지 않아도 아무 영향이 없으면 변화는 일어나지 않는다. 데이터 사고는 규칙만으로 만들어지지 않는다. **보상과 인정**이 있을 때 문화가 바뀐다.

교육은 반복적이고 실무 중심이어야 한다. 예를 들어 “이번 달 데이터 오류 사례”를 공유하고, 이를 어떻게 수정했는지 함께 보는 방식이 효과적이다. 이 과정에서 현업은 데이터 사고를 자신의 업무와 연결해서 이해하게 된다. 교육이 이론에 머물면 변화가 없다. 데이터 사고는 결국 **현장에서 체득되는 습관**이다.

## 21. 한계와 주의점
데이터 사고에도 한계가 있다. 데이터가 없으면 분석할 수 없다. 또한 데이터가 있어도 편향이 있으면 잘못된 결론이 나온다. AI는 이런 편향을 스스로 판단하지 못한다. 따라서 데이터 사고는 “데이터가 말하지 않는 것”을 의식해야 한다. 또한 데이터는 항상 맥락이 필요하다. 숫자만 보면 실무를 놓칠 수 있다. 데이터 사고는 숫자와 현실의 균형을 맞추는 능력이다.

또한 데이터는 과거를 설명하는 데 강하지만, 미래를 보장하지는 않는다. 과거 패턴이 미래에도 그대로 이어진다고 가정하면 위험하다. 특히 시장 변화가 큰 시기에는 데이터가 빠르게 무용지물이 될 수 있다. 따라서 데이터 사고는 “데이터가 알려주는 것과 알려주지 않는 것”을 구분해야 한다. AI는 예측을 도울 수 있지만, 확정적인 답을 주지 않는다.

데이터 사고는 숫자를 맹신하지 않는 태도다. 숫자가 정확해도 해석이 잘못되면 결론은 틀릴 수 있다. 예를 들어 매출 감소가 나쁜 것처럼 보이지만, 전략적으로 의도한 감소일 수도 있다. 데이터는 해석의 결과이며, 해석은 맥락의 결과다. 데이터 사고는 **숫자와 맥락을 함께 보는 태도**다.

또한 데이터는 “설명 책임”과 연결된다. 숫자가 왜 그렇게 나왔는지 설명할 수 있어야 한다. 설명할 수 없는 데이터는 신뢰를 잃는다. AI가 만든 결과도 마찬가지다. 설명할 수 있는 데이터만이 조직에서 살아남는다. 데이터 사고는 결국 **설명 가능한 기준**을 만드는 능력이다.

> 핵심 경고: 숫자가 정확해도 **해석이 틀리면 결론은 틀린다.**

## 22. 다음 장으로의 연결
데이터 사고는 결국 “업무 흐름 설계”로 이어진다. 데이터가 어디서 생성되고, 어디서 쓰이고, 어떻게 전달되는지 이해하면 워크플로우가 보인다. 다음 장에서는 이 데이터를 기반으로 업무 흐름을 설계하고 자동화하는 방법을 다룬다. 데이터 사고는 워크플로우 설계의 기반이다. 이 장이 데이터를 이해하는 눈을 열었다면, 다음 장은 흐름을 설계하는 단계다.

워크플로우 설계는 데이터 이동을 명확히 하는 작업이다. 누가 데이터를 만들고, 누가 검토하며, 어디에 저장하고, 어떻게 공유할지를 정해야 한다. 이 흐름이 명확해지면 자동화 지점도 보인다. 데이터 사고는 자동화를 위한 준비 단계다. 다음 장에서 다룰 워크플로우 설계는 데이터 사고의 자연스러운 확장이다.

실무에서는 간단한 흐름도(예: 입력 → 검토 → 공유 → 보관)만 그려도 문제가 보인다. 어느 단계에서 데이터가 누락되는지, 어느 단계에서 책임이 불명확한지 확인할 수 있다. 이런 흐름도를 팀과 공유하면, 데이터 기준이 자연스럽게 통일된다. 워크플로우 설계는 복잡한 자동화가 아니라 **업무 흐름을 눈에 보이게 만드는 것**에서 시작한다.

## 23. 요약
이 장은 데이터 사고의 기본을 설명했다. 데이터와 정보의 차이를 이해하고, 데이터 라이프사이클과 품질 6요소를 기억해야 한다. 입력 기준과 표준화, 메타데이터와 데이터 사전이 품질을 만든다. 데이터 거버넌스와 정책은 책임을 명확히 한다. AI는 데이터를 소비하지만 책임지지 않는다. 기본 분석, 시각화, 스토리텔링은 실무자에게 가장 중요한 데이터 활용 방식이다. 업종별 맥락과 조직 문화는 데이터 사고의 성패를 좌우한다. 결국 데이터 사고는 기술이 아니라 **기준과 문화**의 문제다.

요약 체크(한 줄씩)

- 기준 없는 숫자는 위험하다
- 입력 규칙이 품질을 만든다
- 데이터는 책임이 있어야 살아남는다

요약하면 데이터 사고는 “기준을 세우고, 기준을 지키는 능력”이다. 이 능력이 있으면 AI가 만든 결과를 신뢰할 수 있다. 기준이 없으면 AI는 혼란을 만든다. 데이터 사고는 AI를 쓰기 위한 기술이 아니라, **업무를 안정적으로 운영하기 위한 사고 방식**이다. 이 장의 핵심은 단순하다. 데이터는 기술이 아니라 기준이며, 그 기준을 지키는 조직이 가장 큰 성과를 얻는다.

또한 데이터 사고는 “지속성”의 문제다. 한 번의 정리는 오래가지 않는다. 데이터는 계속 만들어지고, 계속 변한다. 따라서 기준을 주기적으로 점검하고 업데이트해야 한다. 이 과정을 반복할수록 데이터는 자산이 된다. 데이터 사고는 단기간 프로젝트가 아니라 **지속적 운영 방식**이다. 이 점을 이해하면 AI 활용도 더 안정된다.

마지막으로, 데이터 사고는 AI 활용의 신뢰 기반이다. AI가 아무리 좋아져도 데이터가 불안정하면 결과는 흔들린다. 반대로 데이터가 안정되면 AI는 빠르고 일관된 결과를 만든다. 즉 데이터 사고는 AI를 위한 준비이자, 조직의 기본 역량이다. 이 기반이 있을 때 AI는 **업무의 파트너**가 된다.

한 문장으로 말하면, 데이터 사고는 “업무의 기준을 숫자로 표현하는 능력”이다. 이 능력이 있으면 조직은 더 빠르게 배우고, 더 안정적으로 성장한다. 데이터는 결국 조직의 기억이다. 기억이 정확할수록 AI도 정확해진다.
작은 데이터라도 꾸준히 쌓이면 큰 자산이 된다. 데이터 사고는 그 자산을 길러내는 습관이다. 꾸준함이 결국 경쟁력이 된다.
이 관점을 잃지 않으면 AI 활용은 흔들리지 않는다.
조직의 기준이 곧 데이터의 힘이다.
이것이 핵심이다.

## 24. 용어 풀이
- **메타데이터**: 데이터의 의미, 출처, 생성 시점을 설명하는 정보
- **데이터 사전**: 조직에서 쓰는 용어와 정의를 정리한 문서
- **데이터 스튜어드**: 데이터 품질과 운영을 관리하는 역할
- **CRISP-DM**: 데이터 분석 프로세스 표준
- **데이터 편향**: 특정 집단이 과도하게 반영된 데이터 상태

## 참고/출처
- ISO 8000-1 Data Quality: https://www.iso.org/standard/81747.html
- DAMA-DMBOK: https://www.dama.org/content/body-knowledge
- IBM SPSS Modeler CRISP-DM Guide: https://www.ibm.com/docs/en/spss-modeler/18.3.0?topic=dm-crisp-help-overview
- NIST AI RMF 1.0: https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10
