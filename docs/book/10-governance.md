---
title: 10. 거버넌스와 리스크 관리
---

## 1. 거버넌스는 ‘신뢰의 인프라’다
AI는 빠르고 편리하지만, **신뢰가 없으면 확산되지 않는다**. 거버넌스는 조직이 AI를 안전하고 일관되게 쓰기 위한 규칙과 책임의 체계다. 한마디로 “누가 무엇을 책임지고, 어떤 기준으로 판단하는가”를 문서화하고 운영하는 일이다. 기술 도입보다 먼저 해야 할 일은 이 기준을 세우는 것이다. 기준이 없으면 AI는 조직 내에서 제각각 쓰이고, 결과는 흔들린다. 따라서 거버넌스는 AI 성과를 만드는 토대다.

거버넌스는 단순히 규제를 의미하지 않는다. 오히려 **실무를 빠르게 하기 위한 최소한의 규칙**이다. 기준이 있어야 질문이 줄어든다. 예를 들어 “고객에게 자동 답변을 보내도 되는가?”라는 질문이 반복된다면, 규칙이 없다는 뜻이다. 규칙이 있으면 담당자는 매번 승인받지 않아도 된다. 거버넌스는 속도를 늦추는 게 아니라, 속도를 유지하기 위한 장치다.

신뢰는 내부와 외부 모두에서 필요하다. 내부에서는 구성원이 “이 결과를 믿고 써도 되는가”를 판단해야 하고, 외부에서는 고객과 규제기관이 “이 조직은 안전하게 AI를 쓴다”는 확신을 가져야 한다. 거버넌스는 이 신뢰를 **문서와 기록**으로 보여주는 방법이다. 기록이 남아 있으면 문제가 생겼을 때 원인을 추적할 수 있고, 개선도 가능하다. 기록이 없으면 신뢰는 말로만 남는다.

또 하나의 핵심은 “일관성”이다. 같은 질문에 같은 답이 나와야 조직은 신뢰를 가진다. 거버넌스는 일관성을 만드는 장치다. 예를 들어 고객 응대에서 AI가 서로 다른 기준을 제시하면 혼란이 생긴다. 이 혼란을 막기 위해서는 **기준 문서, 승인 규칙, 업데이트 규칙**이 필요하다. 거버넌스는 결국 “일관된 판단을 만드는 구조”다.

## 2. 실무자에게 거버넌스가 필요한 이유
실무자는 AI의 기술 구조를 몰라도 된다. 그러나 **AI가 업무에 미치는 영향**은 알아야 한다. 잘못된 답변이 고객에게 전달되면 누구의 책임인가, 민감한 정보가 외부로 나가면 누가 대응하는가 같은 질문은 기술 문제가 아니라 경영과 운영의 문제다. 거버넌스는 이런 질문에 대한 답을 정리해준다.

또한 현업 중심 조직은 “누가 기준을 정하는가”가 불분명한 경우가 많다. 거버넌스는 책임자를 명확히 한다. 책임자가 있으면 결정이 빨라지고, 문제 발생 시 대응이 빨라진다. 이것은 현업 중심 조직이 가장 필요로 하는 부분이다. **책임이 분명해지면 업무가 가벼워진다.**

실무에서는 작은 체크리스트만 있어도 효과가 크다. 예를 들어 “이 문서는 외부에 공개되는가?”, “민감 정보가 포함되는가?”, “사람 검토가 필요한가?” 같은 질문 5개만 정해두면 대부분의 판단이 쉬워진다. 실무자는 기술을 이해하지 못해도, 이 체크리스트로 안전하게 사용할 수 있다. 거버넌스는 거대한 체계가 아니라, **현장에서 쓰이는 작은 질문**에서 시작된다.

예를 들어 마케팅팀이 AI로 캠페인 문구를 만들 때도 거버넌스가 필요하다. 문구에 과장 표현이 들어갈 수 있고, 법적 기준을 위반할 수 있기 때문이다. 이런 경우 “법무 검토가 필요한가?”, “외부 공개 전에 승인해야 하는가?” 같은 간단한 규칙만 있어도 위험이 줄어든다. 즉 거버넌스는 특정 부서만의 문제가 아니라, **모든 부서가 공통으로 쓰는 안전 규칙**이다.

## 3. 거버넌스의 기본 원칙(이론)
거버넌스의 핵심 원칙은 단순하다. 복잡한 용어보다 쉽게 설명하면 다음과 같다.

- **책임성(책임의 주인 명확화)**: 누가 승인하고 누가 수정하는지 분명해야 한다.
- **투명성(근거가 보임)**: AI가 왜 그런 답을 했는지 근거를 보여야 한다.
- **안전성(사고를 줄임)**: 잘못된 답이 큰 피해로 이어지지 않게 한다.
- **공정성(특정 집단에 불리하지 않음)**: 편향이 생기지 않도록 관리한다.
- **개인정보 보호(민감 정보 보호)**: 고객·직원 정보는 반드시 보호한다.

이 원칙은 “기술 규칙”이 아니라 “업무 기준”이다. 원칙이 있어야 실무에서 판단이 흔들리지 않는다. 예를 들어 자동화 범위를 정할 때도 “안전성이 충분한가”라는 기준이 있어야 한다.

원칙은 서로 충돌하기도 한다. 예를 들어 속도를 높이려면 자동화를 확대해야 하지만, 안전성을 지키려면 사람 검토가 늘어난다. 이때 필요한 것이 “균형 기준”이다. 중요한 업무에는 안전성을 우선하고, 단순 업무에는 속도를 우선한다는 식으로 정해야 한다. 원칙은 이상적 가치일 뿐 아니라 **실무에서 우선순위를 정하는 기준**이다. 이 기준이 있어야 조직은 갈등 없이 결정을 내릴 수 있다.

실무자에게 도움이 되는 질문으로 원칙을 정리할 수 있다.

- **책임성**: 이 결과에 대해 최종 책임을 지는 사람은 누구인가?
- **투명성**: 이 답의 근거를 다른 사람이 이해할 수 있는가?
- **안전성**: 이 답이 틀리면 피해가 큰가?
- **공정성**: 특정 고객이나 부서에 불리하지 않은가?
- **개인정보 보호**: 이 과정에서 개인 정보가 노출되지 않는가?

이 다섯 가지 질문만으로도 거버넌스의 방향이 잡힌다.

## 4. 거버넌스의 범위와 AI 생애주기
거버넌스는 AI가 만들어지는 순간부터 운영이 끝날 때까지 전체에 적용된다. 이를 **생애주기(처음부터 끝까지)** 관점으로 보면 이해가 쉽다.

- **기획**: 어떤 문제를 풀 것인지 정의한다.
- **데이터 준비**: 데이터를 수집하고 정리한다.
- **모델/도구 선택**: 어떤 AI를 쓸지 결정한다.
- **검토/승인**: 위험을 평가하고 승인한다.
- **운영**: 실제 업무에 사용한다.
- **모니터링**: 성과와 위험을 지속적으로 확인한다.
- **폐기/전환**: 더 이상 필요 없는 AI는 정리한다.

이 흐름을 따라가면 거버넌스가 단순히 “끝에서 검사하는 것”이 아니라 **처음부터 설계되는 것**임을 알 수 있다. 실무자는 이 전체 과정에서 “기획과 승인, 운영과 모니터링”에 특히 관여한다.

생애주기별로 남겨야 할 산출물을 생각하면 더 명확해진다. 예를 들어 기획 단계에서는 “목표와 범위”, 데이터 단계에서는 “데이터 출처와 품질 기준”, 운영 단계에서는 “사용 기록과 성과 지표”가 필요하다. 산출물이 있어야 검토와 개선이 가능하다. 아래는 간단한 예시다.

| 단계 | 핵심 질문 | 남겨야 할 기록 |
|---|---|---|
| 기획 | 무엇을 자동화할 것인가 | 목적/범위 문서 |
| 데이터 | 어떤 데이터를 쓰는가 | 데이터 출처/품질 기준 |
| 검토 | 위험이 무엇인가 | 위험 평가표 |
| 운영 | 성과가 있는가 | 사용 로그/지표 |
| 폐기 | 언제 종료하는가 | 종료 기준/이관 계획 |

이 기록이 쌓이면 조직은 “왜 이 AI를 쓰는지”를 잊지 않는다. 거버넌스는 기록을 통해 기억을 만든다.

생애주기 관점에서 중요한 것은 “종료 기준”이다. AI가 오래된 데이터를 기준으로 답을 내거나, 더 이상 필요하지 않은 업무를 계속 처리하면 오히려 리스크가 커진다. 그래서 “언제 종료할 것인지”를 미리 정해야 한다. 또한 종료 후에는 대체 프로세스가 있어야 한다. AI가 멈추면 업무도 멈추는 상황은 피해야 한다. 거버넌스는 **비상 상황에서도 업무가 유지되는 구조**를 만드는 일이다.

## 5. 운영 모형: 사람·정책·프로세스·기술
거버넌스는 기술만으로 만들어지지 않는다. 가장 현실적인 모형은 네 가지 축으로 정리할 수 있다.

| 축 | 의미 | 예시 |
|---|---|---|
| 사람 | 책임과 역할 | 책임자, 검토자, 승인자 |
| 정책 | 기준과 규칙 | 사용 정책, 금지 영역 |
| 프로세스 | 절차 | 승인 흐름, 사고 대응 |
| 기술 | 도구와 시스템 | 로그, 권한 관리 |

이 네 축이 균형을 이루면 거버넌스는 안정된다. 하나라도 약하면 전체가 흔들린다. 예를 들어 정책은 있지만 담당자가 없으면 실행이 안 되고, 담당자는 있지만 기록 시스템이 없으면 감사가 불가능하다. **거버넌스는 균형의 문제**다.

실무에서는 역할을 **RACI(책임, 승인, 협의, 통보)** 형태로 정리하면 혼란이 줄어든다. 예를 들어 AI 정책 개정은 “책임: 기획팀, 승인: 경영진, 협의: 법무/보안, 통보: 전사”처럼 정리한다. 이처럼 역할이 보이면 실행 속도가 빨라진다. 거버넌스는 결국 사람이 움직이는 구조다.

또한 거버넌스 위원회를 만들지 않더라도, **정기 리뷰 일정**은 필요하다. 분기마다 한 번 “AI 사용 현황과 리스크”를 리뷰하는 자리만 있어도 문제가 쌓이기 전에 발견할 수 있다. 정기 리뷰는 거버넌스의 최소 장치다.

## 6. 정책 체계: 최소한의 규칙부터 시작
정책은 길게 쓰면 아무도 읽지 않는다. 그래서 최소한의 규칙부터 시작하는 것이 현실적이다. 아래는 기본 정책의 예시다.

- **사용 범위 정책**: 어떤 업무에 AI를 사용할 수 있는가
- **금지 영역 정책**: 법무/의료 등 위험도가 높은 영역은 자동화 금지
- **데이터 정책**: 민감 데이터는 외부 AI에 입력 금지
- **공개 정책**: 외부 공개 문서는 반드시 검토 후 배포
- **로그 정책**: 주요 사용 기록은 반드시 남긴다

정책은 조직의 규모와 위험에 맞게 조정하면 된다. 중요한 것은 “작게 시작해서 지속하는 것”이다. 정책은 시간이 지나면 수정해야 한다. 변화가 생겼을 때 업데이트할 책임자도 정해둬야 한다.

정책 문서는 길지 않아야 한다. 보통 한두 페이지면 충분하다. 구조도 간단하면 된다. 예를 들어 다음과 같은 틀을 쓰면 이해가 쉽다.

| 항목 | 내용 예시 |
|---|---|
| 목적 | 안전하고 신뢰 가능한 AI 활용 |
| 적용 범위 | 전사 AI 사용 |
| 금지 사항 | 민감 정보 입력 금지 |
| 승인 절차 | 위험 등급별 승인 |
| 예외 처리 | 책임자 승인 시 제한적 허용 |

이 구조는 누구나 읽고 이해할 수 있다. 정책은 “복잡한 설명서”가 아니라 **현장에서 바로 쓰는 규칙**이어야 한다.

정책에는 예외 처리 규칙이 반드시 포함되어야 한다. 예외가 없으면 현장은 우회하려고 하고, 거버넌스는 무너진다. 예를 들어 “긴급 상황에서는 책임자 승인 후 임시 사용 가능” 같은 문구가 있으면 현실이 된다. 또한 정책은 단순히 문서로 배포하면 끝이 아니다. **짧은 교육과 반복 안내**가 필요하다. 정책이 현장에 스며들어야 실효성이 생긴다.

정책을 만들 때 유용한 방식은 “3층 구조”다.

- **원칙**: 왜 이 규칙이 필요한가(가치)
- **규칙**: 무엇을 해야 하는가(행동)
- **예외**: 언제 유연하게 적용할 것인가(현실)

이 구조는 규칙을 지키면서도 현장의 유연성을 확보한다. 실무자에게 가장 중요한 것은 이 균형이다.

## 7. 리스크 유형과 평가 모형
리스크는 종류별로 나눠보면 관리가 쉬워진다. 실무자 관점에서 중요한 리스크는 다음과 같다.

| 리스크 유형 | 설명 | 예시 |
|---|---|---|
| 법적 리스크 | 규정 위반 | 개인정보 유출 |
| 운영 리스크 | 업무 장애 | 잘못된 답변으로 작업 중단 |
| 평판 리스크 | 신뢰 하락 | 고객 불만 확대 |
| 보안 리스크 | 시스템 공격 | 프롬프트 인젝션(질문 조작) |
| 재무 리스크 | 비용 증가 | 불필요한 AI 사용 비용 |

평가 모형은 간단한 2축으로도 충분하다. **가능성(높음/중간/낮음)**과 **영향(큼/중간/작음)**을 표로 나누면 된다. 이 방식은 복잡한 수치보다 현실적인 판단을 돕는다.

| 영향 \ 가능성 | 낮음 | 중간 | 높음 |
|---|---|---|---|
| 작음 | 낮은 위험 | 낮은 위험 | 중간 위험 |
| 중간 | 낮은 위험 | 중간 위험 | 높은 위험 |
| 큼 | 중간 위험 | 높은 위험 | 매우 높은 위험 |

이 표를 기반으로 “어떤 경우에 사람 검토가 필요한지” 같은 규칙을 정할 수 있다.

리스크는 서로 연결되어 있다는 점도 중요하다. 예를 들어 데이터 유출은 곧 법적 리스크와 평판 리스크로 연결된다. 따라서 하나의 리스크가 발생하면 연쇄적으로 문제를 만들 수 있다. 이 연결을 이해하면, 작은 오류도 쉽게 넘기지 않게 된다. 거버넌스는 단일 리스크가 아닌 **연쇄 리스크**를 관리하는 작업이다.

리스크 평가를 실무에 적용할 때는 “사례 기반 점검”이 효과적이다. 예를 들어 고객센터 자동 답변을 도입할 때, 과거에 발생했던 민원 사례를 꺼내어 “이 상황에서 AI가 답했을 때 위험은 무엇인가”를 검토한다. 이런 사례 검토는 추상적인 위험을 구체화해준다. 또한 위험은 시간에 따라 달라진다. 정책이 바뀌거나 규제가 강화되면 리스크가 높아질 수 있다. 그래서 **정기적인 재평가**가 필요하다.

## 8. 위험 대응 전략
리스크를 알았으면 대응을 정해야 한다. 대응 전략은 네 가지로 단순화할 수 있다.

- **회피**: 위험이 너무 크면 해당 사용을 하지 않는다.
- **완화**: 통제 장치를 두어 위험을 줄인다.
- **이전**: 보험, 외부 전문기관 활용으로 부담을 나눈다.
- **수용**: 위험을 감수하되 모니터링한다.

예를 들어 고객 민감 정보가 들어간 업무는 회피 또는 강한 완화가 필요하다. 반면 내부 요약 업무는 수용이 가능하다. 이 판단은 기술이 아니라 **업무 중요도와 책임**에 기반한다.

조직은 “허용할 수 있는 위험 수준(리스크 허용치)”을 정해야 한다. 허용치가 없으면 매번 논쟁이 반복된다. 예를 들어 고객에게 직접 영향을 주는 업무는 허용치를 낮게 잡고, 내부 참고용 업무는 허용치를 높게 잡는 식이다. 허용치는 경영진과 실무자 간의 합의가 필요하다. 이 합의가 있으면 결정 속도가 빨라진다. 거버넌스는 **합의된 위험 기준** 위에서 움직인다.

## 9. AI 시스템 인벤토리와 사용 사례 분류
조직에서 어떤 AI를 쓰고 있는지 모르면 거버넌스를 할 수 없다. 그래서 **AI 인벤토리(목록)**가 필요하다. 인벤토리는 간단한 표로 시작해도 충분하다.

| 항목 | 내용 예시 |
|---|---|
| 시스템명 | 고객 FAQ 봇 |
| 담당 부서 | 고객센터 |
| 사용 목적 | 문의 응답 자동화 |
| 데이터 종류 | 고객 문의(일반) |
| 위험 등급 | 중간 |
| 승인자 | 책임자 이름 |

인벤토리가 만들어지면 “어떤 업무가 위험도가 높은지” 한눈에 보인다. 또한 인벤토리는 규제 대응에도 유용하다. 실무자 관점에서는 “내가 쓰는 AI가 목록에 있는가”를 확인하는 것이 시작이다.

인벤토리는 한 번 만들고 끝내면 안 된다. 신규 도구가 생기거나, 부서가 자체적으로 AI를 사용할 때마다 업데이트해야 한다. 그래서 **업데이트 책임자**가 필요하다. 월 1회 또는 분기 1회라도 인벤토리를 점검하면 누락을 줄일 수 있다. 또한 인벤토리는 “기술 목록”이 아니라 **업무 목록**이어야 한다. 같은 모델을 쓰더라도 업무가 다르면 위험이 다르기 때문이다. 즉 “도구 기준”이 아니라 “업무 기준”으로 관리해야 한다.

인벤토리에 추가하면 좋은 항목도 있다. 예를 들어 “사용 빈도”, “외부 공개 여부”, “자동화 정도” 같은 항목이다. 이 항목은 위험 판단을 더 정확하게 해준다. 또한 인벤토리는 신규 직원 교육에도 활용된다. “우리 조직은 어떤 AI를 어떤 기준으로 쓰는가”를 한눈에 보여주기 때문이다. 인벤토리는 거버넌스의 지도 역할을 한다.

## 10. 사용 사례 등급과 승인 게이트
모든 AI 사용을 동일하게 취급하면 비효율적이다. 그래서 **사용 사례 등급**을 나누는 것이 필요하다. 예시는 다음과 같다.

- **낮은 위험**: 내부 메모 요약, 일정 정리
- **중간 위험**: 내부 정책 질의응답, 보고서 초안 작성
- **높은 위험**: 고객 의사결정, 법적 문서 생성
- **금지**: 생명/안전 판단 자동화

등급을 나누면 승인 절차도 달라진다. 낮은 위험은 간단 승인, 높은 위험은 다중 검토가 필요하다. 이런 게이트가 있어야 조직은 안전하게 확산할 수 있다. 게이트는 속도를 늦추기 위한 것이 아니라, **위험에 맞는 속도를 유지하기 위한 장치**다.

승인 게이트를 설계할 때는 “필수 질문”을 정해두는 것이 효과적이다. 예를 들어 다음과 같은 질문이다.

- 결과가 고객에게 직접 전달되는가?
- 민감 정보가 포함되는가?
- 자동화가 잘못되면 비용이 큰가?
- 기존 정책과 충돌하는가?

이 질문 중 하나라도 “예”라면 강화된 검토가 필요하다. 기준이 명확하면 승인 절차가 빠르게 진행된다. 기준이 없으면 매번 논쟁이 반복된다.

승인 게이트는 문서 흐름과 연결해야 한다. 예를 들어 보고서 작성 시 “AI 사용 여부” 체크를 넣고, 사용했으면 자동으로 검토 요청이 생성되도록 한다. 이렇게 하면 규칙이 자동으로 적용된다. 반대로 별도의 승인 시스템만 만들어두면 현장은 이를 우회하게 된다. 거버넌스는 **업무 흐름 속에 녹아들어야** 지속된다.

## 11. 데이터 거버넌스: 데이터가 위험을 만든다
AI의 품질과 위험은 데이터에 달려 있다. 그래서 데이터 거버넌스가 핵심이다. 실무자도 이해할 수 있는 기본 규칙은 다음과 같다.

- **정확성**: 데이터가 최신이며 오류가 없는가
- **대표성**: 특정 집단에 편향되지 않았는가
- **보안성**: 민감 정보가 섞여 있지 않은가
- **보존 정책**: 어느 기간까지 보관할 것인가

데이터 정책이 없으면 AI는 잘못된 답을 낸다. 그리고 그 책임은 결국 조직이 져야 한다. 데이터는 보이지 않는 위험의 근원이다. 그래서 **데이터 관리가 곧 리스크 관리**다.

데이터 거버넌스에서 중요한 개념 중 하나는 **데이터 계보(어디서 왔는가)**다. 데이터 출처가 불분명하면 오류를 추적할 수 없다. 또한 “데이터 최소화(필요한 만큼만 사용)” 원칙이 있어야 한다. 모든 데이터를 다 넣으면 위험이 커진다. 필요한 데이터만 사용하면 위험도 줄고 품질 관리도 쉬워진다. 실무자 관점에서 할 수 있는 가장 큰 일은 “필요하지 않은 데이터는 넣지 않는 것”이다.

데이터 품질을 높이기 위해 간단한 점검표를 만들 수 있다.

- 데이터가 최신인가?
- 중복이나 누락이 없는가?
- 개인 정보가 포함되어 있는가?
- 출처가 명확한가?
- 업데이트 책임자가 있는가?

이 점검표는 기술 지식이 없어도 사용할 수 있다. 데이터는 거버넌스의 시작이자 끝이다. 데이터가 흔들리면 AI 품질도 흔들린다.

또한 **보관 기간과 삭제 기준**을 정하는 것이 중요하다. 데이터는 쌓일수록 위험도 커진다. “언제까지 보관하고, 언제 삭제할 것인가”가 명확해야 한다. 특히 개인정보가 포함된 데이터는 보관 기간이 지나면 안전하게 삭제해야 한다. 이 기준이 없으면 나중에 큰 문제가 된다. 데이터 거버넌스는 결국 **데이터를 관리하는 약속**이다.

## 12. 모델·프롬프트·버전 관리
실무자도 알아야 할 중요한 개념은 **버전 관리**다. AI가 쓰는 모델이나 프롬프트가 바뀌면 결과가 달라질 수 있다. 따라서 최소한의 기록이 필요하다.

- 어떤 모델을 썼는가
- 어떤 프롬프트를 썼는가
- 언제 변경했는가
- 누가 승인했는가

특히 프롬프트 인젝션(질문 조작) 같은 위험을 줄이기 위해서는 **표준 프롬프트 템플릿**을 만드는 것이 좋다. 템플릿을 쓰면 누가 사용하든 결과가 일정해지고, 위험 관리도 쉬워진다. 버전 기록은 책임을 지는 데 필수다.

프롬프트 템플릿에는 “역할, 범위, 금지 사항”을 명시하는 것이 좋다. 예를 들어 “당신은 고객센터 직원이며, 내부 정책 문서만을 근거로 답한다”처럼 역할을 지정한다. 또 “추측하지 말고 모르면 모른다고 답한다” 같은 규칙을 넣으면 위험이 줄어든다. 프롬프트는 기술자가 아니어도 작성할 수 있다. **업무 기준을 문장으로 옮기는 작업**이기 때문이다.

프롬프트와 모델의 변경은 “작은 업데이트”처럼 보여도 결과에 큰 영향을 준다. 그래서 변경 기록과 승인 규칙이 필요하다. 예를 들어 템플릿을 수정할 때는 간단한 리뷰를 거치고, 이전 버전으로 되돌릴 수 있는 **롤백(되돌리기)** 절차를 마련한다. 또한 템플릿을 한곳에 모아 “프롬프트 라이브러리”를 만들면 품질이 올라간다. 라이브러리를 쓰면 각 부서가 제각각 만들지 않아도 되고, 거버넌스가 쉬워진다.

## 13. 평가와 지표(성과를 보이는 방법)
거버넌스는 숫자가 없으면 유지되지 않는다. 최소한의 지표를 정해두면 된다.

- **정확도**: 답이 맞는가
- **안전성 지표**: 민감 정보 누락 여부
- **처리 시간**: 업무 시간이 줄었는가
- **불만 건수**: 사용자 불만이 늘었는가
- **오류 보고 건수**: 문제가 몇 건 발생했는가

이 지표는 “관리의 나침반”이다. 지표가 없으면 개선이 불가능하다. 그러나 지표를 너무 많이 잡으면 운영 부담이 커진다. **핵심 3~5개만 유지**하는 것이 현실적이다.

지표를 운영하는 방법은 간단하다. 먼저 현재 수준을 측정해 “기준선(베이스라인)”을 만든다. 그 다음 “다음 분기에는 불만 건수를 20% 줄인다”처럼 현실적인 목표를 잡는다. 지표는 단순히 숫자가 아니라 **개선 목표**여야 한다. 숫자를 모으는 것 자체가 목적이 되면 실무가 지친다. 따라서 지표는 “행동을 바꾸는 도구”로 써야 한다.

지표를 쉽게 보기 위해 간단한 표를 만들면 좋다.

| 지표 | 현재 | 목표 | 주기 |
|---|---|---|---|
| 답변 정확도 | 82% | 90% | 월간 |
| 민감 정보 노출 | 3건 | 0건 | 월간 |
| 처리 시간 | 12분 | 8분 | 분기 |
| 사용자 불만 | 15건 | 10건 | 월간 |

이 표는 실제 수치를 의미하기보다, “어떤 지표를 관리할지”를 보여주는 틀이다. 숫자는 조직 상황에 맞게 설정하면 된다.

지표를 해석할 때는 “숫자만 보는 함정”을 피해야 한다. 예를 들어 답변 정확도가 높아졌는데 사용자 불만이 늘었다면, 답변이 길어져서 이해가 어려워졌을 수 있다. 그래서 지표는 **함께 보는 것이 중요**하다. 하나의 지표만으로 판단하면 왜곡이 생긴다. 거버넌스는 숫자를 모으는 것이 아니라, **숫자를 통해 대화를 시작하는 것**이다.

## 14. 모니터링·로그·감사
거버넌스는 “기록이 남는 구조”여야 한다. 로그가 없으면 문제를 되돌아볼 수 없다. 기본적으로 기록해야 할 항목은 다음과 같다.

- 누가 사용했는가
- 어떤 질문을 했는가
- 어떤 근거 문서를 썼는가
- 어떤 답을 냈는가
- 오류나 수정이 있었는가

이 기록은 감사(검토)에서 중요한 근거가 된다. 로그는 기술 문제처럼 보이지만, 실제로는 **책임과 투명성**을 보장하는 장치다. 기록이 없으면 거버넌스는 이름만 남는다.

로그를 남길 때는 개인정보를 포함하지 않도록 주의해야 한다. 질문 내용이 민감 정보를 포함할 수 있기 때문이다. 그래서 로그에는 “질문 요약”만 남기고 원문은 제한적으로 보관하는 방식도 있다. 또한 로그 보관 기간을 정해야 한다. 너무 길게 보관하면 보안 위험이 커지고, 너무 짧으면 감사가 어렵다. 조직의 성격에 맞는 기간을 정하는 것이 필요하다. **로그는 남기는 것만큼 지키는 것도 중요**하다.

로그는 단순히 저장하는 데서 끝나지 않는다. 월간 또는 분기별로 “로그 리뷰 리포트”를 만들면 개선이 쉬워진다. 예를 들어 어떤 질문이 반복적으로 들어오는지, 어느 부서에서 오류가 많이 발생하는지 확인할 수 있다. 이런 리포트는 거버넌스가 실제로 작동하고 있다는 증거가 된다. 로그는 **운영 개선의 재료**다.

감사 준비를 위해서는 “누가 언제 로그를 본다”는 규칙도 필요하다. 로그를 남기기만 하고 아무도 보지 않으면 의미가 없다. 담당자가 정기적으로 리뷰하고, 이상 징후가 보이면 즉시 조치해야 한다. 예를 들어 특정 부서에서 민감 정보가 반복 입력된다면 교육을 강화하거나 입력 제한을 추가해야 한다. 로그는 **문제를 조기에 발견하는 경보 시스템**이어야 한다.

## 15. 사람의 검토와 책임 체계
AI의 답은 항상 옳지 않다. 그래서 중요한 업무에는 **사람의 검토(HITL, 사람이 확인하는 절차)**가 필요하다. 예를 들어 다음 조건이라면 반드시 사람 검토가 들어가야 한다.

- 고객에게 직접 전달되는 답변
- 법적 책임이 생길 수 있는 문서
- 안전과 관련된 판단

사람 검토를 넣으면 속도가 느려질 수 있지만, 사고 위험은 크게 줄어든다. 중요한 것은 “어떤 경우에 검토가 필요한지”를 미리 정하는 것이다. 이 기준이 없으면 현장은 혼란을 겪는다.

검토 절차는 단순해야 한다. 예를 들어 “중간 위험은 1단계 검토, 높은 위험은 2단계 검토”처럼 단계만 정해도 효과가 있다. 또한 검토 결과를 기록해야 한다. 기록이 있어야 재발 방지가 가능하다. 검토는 누군가의 부담이 아니라, **책임을 분산하고 사고를 줄이는 장치**다. 검토가 잘 작동하면 조직의 신뢰가 높아진다.

검토 과정에는 “이의 제기(피드백) 통로”도 필요하다. 사용자가 AI 답변에 문제가 있다고 느끼면 쉽게 보고할 수 있어야 한다. 이 피드백이 모이면 어떤 영역이 위험한지 빠르게 파악할 수 있다. 또한 일정 수준 이상의 오류가 감지되면 자동으로 사용을 중지하거나 제한하는 규칙을 둘 수도 있다. 거버넌스는 **피드백과 중단 기준**을 함께 갖춰야 한다.

## 16. 보안 위협과 통제
AI는 새로운 보안 위험을 만든다. 실무자가 알아야 할 대표적인 위험과 대응은 아래와 같다.

| 위협 | 설명 | 대응 |
|---|---|---|
| 프롬프트 인젝션 | 질문에 악의적 명령을 심어 답을 왜곡 | 입력 필터링, 템플릿 사용 |
| 데이터 유출 | 민감 정보가 답변에 포함 | 접근 제어, 마스킹 |
| 모델 오남용 | 규정 밖 사용 | 사용 정책, 로그 감사 |
| 공급망 위험 | 외부 서비스 문제 | 벤더 검토, 계약 조항 |

보안은 기술보다 **규칙과 습관**이 중요하다. 직원이 민감 정보를 넣지 않는 것부터 시작해야 한다.

또한 보안은 “교육”과 함께 가야 한다. 직원이 위험을 이해하지 못하면 규칙이 지켜지지 않는다. 예를 들어 “고객 주민번호는 절대 입력하지 않는다”는 규칙을 교육으로 반복해야 한다. 작은 교육이 큰 사고를 막는다. 보안은 거창한 시스템보다 **현장 습관**에서 시작된다.

보안 위험을 줄이기 위해 **모의 공격(레드팀 테스트)**을 해볼 수 있다. 이는 실제 공격을 흉내 내어 취약점을 찾는 과정이다. 예를 들어 프롬프트 인젝션을 시도해보고, 시스템이 이를 어떻게 막는지 확인한다. 실무자라도 “테스트 질문 목록”을 만들고 결과를 확인할 수 있다. 작은 테스트라도 문제를 미리 발견하면 큰 사고를 막을 수 있다.

사고 대응 절차도 마련해야 한다. 예를 들어 “민감 정보가 노출되면 즉시 사용 중지 → 관련 부서 통보 → 원인 분석 → 재발 방지” 같은 단계다. 대응 절차가 없으면 사고가 커진다. 또한 대외 커뮤니케이션 기준도 필요하다. 고객에게 어떤 내용을 언제 알릴지 미리 정해야 혼란을 줄일 수 있다. 보안은 예방뿐 아니라 **대응 준비**까지 포함한다.

## 17. 국제 표준과 제도 흐름
AI는 전 세계적으로 규제와 표준이 강화되고 있다. 실무자에게 중요한 것은 “기술이 아니라 규범의 대상”이라는 사실이다. 대표적인 흐름은 다음과 같다.

- **NIST AI RMF**: 위험 관리 원칙을 제시
- **ISO/IEC 42001**: AI 경영 시스템 표준
- **ISO/IEC 23894**: AI 리스크 관리 지침
- **EU AI Act**: 위험 수준에 따른 규제 구조
- **OECD/UNESCO 원칙**: 공정성과 신뢰 기반 원칙

이 기준을 모두 외울 필요는 없다. 중요한 것은 “내 조직의 정책이 국제 기준과 큰 방향이 맞는가”를 확인하는 것이다. 최소한의 방향성을 맞추면 규제 대응이 쉬워진다.

대표 기준은 공통적으로 “책임, 안전, 투명성”을 강조한다. 따라서 조직은 이 세 가지에 맞는 정책만 갖추어도 큰 틀에서 어긋나지 않는다. 예를 들어 NIST AI RMF는 **Govern-Map-Measure-Manage**라는 흐름을 제시한다. 이는 거버넌스의 운영 흐름과 유사하다. 실무자는 세부 조항보다 **큰 방향**을 이해하는 것이 중요하다.

또한 ISO 경영시스템은 **계획-실행-점검-개선(Plan-Do-Check-Act)** 구조를 강조한다. 이 구조를 거버넌스에 적용하면 자연스럽게 운영 루프가 만들어진다. 즉 정책을 만들고(Plan), 운영하고(Do), 지표로 점검하고(Check), 개선하는(Act) 순환을 만들면 된다. 이 방식은 복잡한 기술 지식 없이도 적용 가능하다.

규제 흐름은 국경을 넘는다. 특히 데이터가 해외 서버로 이동할 경우, **데이터 주권(국가별 데이터 규정)** 문제가 생길 수 있다. 따라서 국제 표준뿐 아니라 “데이터가 어디에 저장되는가”를 확인해야 한다. 이는 벤더 선정과 계약에도 영향을 준다. 실무자라도 “데이터가 어디로 가는지”를 묻는 습관이 필요하다. 거버넌스는 법적 리스크를 줄이는 첫 단계다.

## 18. 벤더(외부 서비스) 관리
AI는 외부 서비스를 많이 활용한다. 이때 위험은 “내가 통제할 수 없는 부분”에서 발생한다. 벤더 관리는 최소한 다음을 확인해야 한다.

- 데이터 저장 위치와 보관 기간
- 모델 학습에 데이터가 사용되는지 여부
- 보안 인증 또는 감사 결과
- 장애 발생 시 대응 절차
- 계약서에 책임 범위가 명시되어 있는지

벤더 관리는 법무나 구매 부서와 협업이 필요하다. 기술만으로 해결할 수 없다. **외부 서비스를 쓰는 순간 거버넌스는 계약이 된다.**

또한 “출구 전략”이 필요하다. 특정 벤더에 과도하게 의존하면 가격 상승이나 서비스 중단에 취약해진다. 그래서 계약 시 **데이터 반출 가능 여부**, **대체 서비스로 이전할 수 있는지**를 확인해야 한다. 벤더 관리는 단기 비용이 아니라 장기 리스크를 줄이는 작업이다.

벤더 계약에는 **SLA(서비스 수준 약속)**와 **책임 범위**가 포함되어야 한다. 예를 들어 장애 발생 시 복구 시간, 데이터 유출 발생 시 책임 범위 등이 명확히 정의되어야 한다. 이런 조항이 없으면 사고가 발생했을 때 책임 소재가 모호해진다. 실무자라도 계약서에 이러한 항목이 있는지 확인해야 한다.

## 19. 범용 사례 + 업종별 박스
다음은 실무에서 많이 쓰이는 범용 사례와 업종별 포인트다.

**범용 사례**
- 내부 정책 질의응답: 최신 규정 문서 기반 답변
- 보고서 초안 작성: 데이터 정리 후 초안 생성
- 고객 FAQ 자동화: 반복 문의 자동 답변

**업종별 박스**

| 업종 | 리스크 포인트 | 거버넌스 중점 |
|---|---|---|
| 금융 | 규제 준수, 모델 리스크 | 승인/감사 기록 강화 |
| 의료 | 환자 안전, 개인정보 | 비식별화, 이중 검토 |
| 제조 | 안전 절차 | 현장 검증 프로세스 |
| 공공 | 정책 신뢰성 | 근거 문서 필수 |
| 유통/서비스 | 고객 불만 | 답변 품질 검수 |

업종별로 리스크의 무게가 다르다. 따라서 거버넌스의 깊이도 달라져야 한다. 모든 업종에 동일한 규칙을 적용하면 실패한다.

조직 내부에서도 부서별로 위험이 다르다. 예를 들어 인사팀은 개인정보 관리가 핵심이고, 영업팀은 고객 대응의 신뢰가 핵심이다. 그래서 부서 단위의 작은 규칙을 추가하는 것이 좋다. 전사 정책 위에 **부서별 보완 규칙**을 두면 현장이 더 잘 움직인다.

실무 적용 흐름을 간단히 정리하면 다음과 같다.

- 먼저 “가장 위험한 업무”를 고른다.
- 해당 업무의 문서와 데이터를 정리한다.
- 사용 규칙과 검토 기준을 만든다.
- 작은 범위로 시험 적용한다.
- 결과를 바탕으로 확장한다.

이 흐름은 업종에 관계없이 적용할 수 있는 기본 패턴이다.

짧은 사례를 하나 더 들어보자. 한 유통 기업이 반품 정책 문의를 자동화하려고 한다. 먼저 반품 정책 문서를 최신 버전으로 정리하고, 승인자를 지정한다. 그 다음 AI 답변에 “근거 문서명과 버전”을 표시한다. 마지막으로 고객 불만이 5건 이상 누적되면 자동 응답을 중지하고 사람 검토로 전환한다. 이 작은 규칙만으로도 신뢰가 크게 올라간다. **거버넌스는 작은 장치의 합**이다.

## 20. 운영 로드맵
거버넌스는 한 번에 완성되지 않는다. 다음과 같은 단계가 현실적이다.

1) **기본 정책 작성**: 최소한의 사용 규칙 정리
2) **인벤토리 구축**: 어떤 AI를 쓰는지 목록화
3) **위험 평가**: 높은 위험 영역 우선 관리
4) **모니터링 체계 구축**: 로그와 지표 운영
5) **지속 개선**: 분기별 리뷰와 정책 업데이트

이 로드맵은 기술 프로젝트가 아니라 **운영 프로젝트**다. 작은 규칙을 만들고, 그것을 반복적으로 지키는 것이 핵심이다.

로드맵을 실행할 때는 변화 관리를 함께 해야 한다. 구성원들은 새로운 규칙을 부담스럽게 느낄 수 있다. 그래서 “왜 필요한지”를 반복적으로 설명하고, 작은 성공 사례를 공유하는 것이 중요하다. 예를 들어 “정책 질문 응답 시간이 30% 줄었다” 같은 성과를 알리면 거버넌스의 가치가 이해된다. 사람의 행동이 바뀌어야 거버넌스가 작동한다.

또한 교육은 짧고 실용적으로 해야 한다. 긴 교육은 피로감을 만든다. 대신 “금지 사항 5가지”, “승인 절차 3단계”처럼 기억하기 쉬운 형태로 전달하는 것이 좋다. 거버넌스가 성공하려면 **작은 규칙을 반복적으로 알리는 것**이 중요하다.

로드맵의 마지막은 “정착”이다. 규칙이 자연스럽게 쓰이도록 업무 절차에 포함시키고, 정기적으로 리뷰해야 한다. 예를 들어 분기 리뷰에서 “AI 사용 사례 3건을 점검하고 개선 사항을 기록한다” 같은 루틴을 만들면 된다. 이런 루틴이 쌓이면 거버넌스는 조직의 문화가 된다. 거버넌스는 결국 **습관으로 굳어져야** 지속된다.

## 21. 한계와 주의점
거버넌스에는 비용이 든다. 규칙을 만들고, 검토하고, 기록하는 데 시간이 필요하다. 그래서 **속도와 안전 사이 균형**이 중요하다. 또 하나의 위험은 “규칙이 형식만 남는 것”이다. 규칙이 있으나 지켜지지 않으면 오히려 신뢰가 떨어진다. 따라서 규칙은 실행 가능한 수준으로 단순해야 한다.

또한 거버넌스는 모든 위험을 없애지 못한다. AI는 불확실성을 가진 기술이다. 그래서 중요한 결정은 항상 사람이 최종 판단을 해야 한다. 거버넌스는 “책임을 AI에 넘기지 않는 장치”임을 잊지 말아야 한다.

또 하나의 주의점은 “형식만 남는 거버넌스”다. 문서는 있지만 실제로는 쓰이지 않는 경우가 많다. 이를 방지하려면 정책을 “업무 흐름에 연결”해야 한다. 예를 들어 보고서 승인 과정에 AI 사용 여부를 체크하는 항목을 넣으면, 정책이 자연스럽게 적용된다. 거버넌스는 문서가 아니라 **업무 과정 속에 있어야** 실효성이 생긴다.

마지막으로, 거버넌스는 완벽함을 목표로 하면 실패한다. 모든 규칙을 한 번에 만들 수는 없다. 중요한 것은 “지금 당장 필요한 규칙부터 시작”하는 것이다. 작은 규칙이 쌓이면 조직의 신뢰가 올라간다. 거버넌스는 거대한 프로젝트가 아니라 **꾸준한 운영 습관**이다.

속도와 안전의 균형은 항상 고민해야 한다. 너무 느리면 현장은 AI를 쓰지 않게 되고, 너무 빠르면 사고 위험이 커진다. 그래서 “업무 중요도에 따라 속도를 다르게 하는 것”이 현실적인 해법이다. 단순한 업무는 빠르게, 중요한 업무는 신중하게. 이 기준이 명확하면 현장은 납득하고 움직인다.

## 22. 요약
이 장은 거버넌스와 리스크 관리의 핵심을 다뤘다. 거버넌스는 신뢰를 만드는 인프라이며, 실무자에게는 책임과 기준을 명확히 하는 장치다. 정책 체계, 리스크 분류, 평가 모형, 인벤토리, 승인 게이트, 데이터 관리, 보안 통제가 기본 요소다. 국제 표준과 규제 흐름은 방향성을 제시하고, 업종별 리스크는 깊이를 결정한다. 결국 거버넌스는 기술보다 **규칙과 책임**의 문제다.

한 문장으로 정리하면, 거버넌스는 “AI를 안전하게 쓰기 위한 약속을 만드는 일”이다. 약속은 문서로 끝나지 않고, 운영과 교육, 기록으로 이어져야 한다. 이 약속이 지켜질 때 조직은 AI를 두려워하지 않고, 안정적으로 활용할 수 있다.

작은 규칙을 반복해서 지키는 조직만이 큰 리스크를 줄이고, 신뢰를 쌓는다.

이 관점이 앞으로의 AI 내재화 로드맵을 이해하는 기준이 된다.

핵심은 지속 가능한 운영이다.

## 23. 용어 풀이
- **거버넌스**: 조직 내 기준과 책임 체계
- **리스크 관리**: 위험을 식별·평가·완화하는 과정
- **HITL**: 사람이 최종 확인하는 절차
- **프롬프트 인젝션**: 질문에 악성 지시를 섞어 답을 왜곡하는 공격
- **AI 인벤토리**: 사용 중인 AI 목록

## 참고/출처
- NIST AI RMF 1.0: https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10
- NIST AI RMF Generative AI Profile: https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-generative-artificial-intelligence
- ISO/IEC 42001:2023: https://www.iso.org/standard/42001
- ISO/IEC 23894:2023: https://www.iso.org/standard/77304.html
- ISO/IEC 42005:2025: https://www.iso.org/standard/81230.html
- EU AI Act 요약(EUR-Lex): https://eur-lex.europa.eu/summary/EN/4762484
- OECD AI Principles: https://oecd.ai/principles/
- UNESCO Recommendation on the Ethics of AI: https://unesdoc.unesco.org/ark:/48223/pf0000380455
