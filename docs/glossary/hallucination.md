---
title: 환각(Hallucination)
---

[← 용어 사전](../glossary)

## 한 줄 정의
환각(Hallucination)은 AI가 사실처럼 보이지만 실제로는 틀린 정보를 자신 있게 생성하는 현상이다.

## 왜 중요한가(실무)
환각은 AI 도입에서 가장 빈번하고 위험한 실패 모드다.
문제는 AI가 "모릅니다"라고 하지 않고, 그럴듯한 문장으로 거짓 정보를 제시한다는 점이다.
존재하지 않는 법률 조항을 인용하거나, 없는 제품 사양을 만들어내거나, 허구의 통계를 제시하는 사례가 실제로 보고되고 있다.

고객 대면 서비스, 법률·의료·금융 자문, 공식 보고서 작성 등 정확성이 필수인 업무에서 환각이 발생하면 신뢰 손실, 법적 책임, 재무적 피해로 이어질 수 있다.
따라서 AI 시스템 설계 시 환각을 "버그"가 아닌 "구조적 특성"으로 인식하고, 이를 전제한 안전장치를 반드시 마련해야 한다.

## 핵심 이론(직관)
### 1) AI는 '확률적으로 자연스러운 다음 단어'를 생성한다
LLM은 사실 데이터베이스가 아니라 패턴 생성기다.
"다음에 올 가장 자연스러운 단어"를 확률적으로 선택하기 때문에, 학습 데이터에 없는 내용도 문법적·문맥적으로 그럴듯하게 이어 붙인다.
이것이 환각의 근본 원인이다.

### 2) 자신감 수준과 정확도는 별개다
AI의 답변 톤이 확신에 차 있다고 해서 정확한 것은 아니다.
모델은 "확실하지 않다"는 내부 신호를 출력 문체에 잘 반영하지 못하며, 틀린 답변도 옳은 답변과 동일한 톤으로 제시하는 경향이 있다.

## 실무 포인트
### 1) RAG(검색 증강 생성)로 근거를 강제하라
모델이 자체 지식에만 의존하지 않도록, 관련 문서를 검색해서 컨텍스트로 제공하는 RAG 구조를 도입한다.
"이 문서 내용만을 근거로 답변하라"는 시스템 프롬프트와 함께 사용하면 환각 빈도를 크게 줄일 수 있다.
단, RAG 자체의 검색 품질이 낮으면 잘못된 근거를 기반으로 한 환각이 발생하므로 검색 정확도도 함께 관리해야 한다.

### 2) 출력 검증 단계를 프로세스에 내장하라
환각을 완전히 제거하는 것은 현재 기술로 불가능하다.
따라서 AI 출력물에 대한 사람의 검토 단계를 업무 프로세스에 명시적으로 포함시켜야 한다.
고위험 업무에서는 "AI 초안 → 담당자 검증 → 최종 승인"의 삼중 구조가 권장된다.

## 체크리스트
- AI 출력이 고객·외부에 직접 노출되는 경로가 있는가 (있다면 검증 단계 필수)
- RAG를 적용한 경우, 검색된 근거 문서가 출력과 함께 표시되는가
- 환각 발생 빈도를 측정할 수 있는 평가(Eval) 체계가 있는가
- 고위험 업무(법률, 의료, 재무)에서 사람 검증 없이 AI 출력이 최종 결과로 사용되는 경우가 없는가
- 환각 사례가 발견되었을 때 피드백하고 프롬프트·파이프라인을 개선하는 루프가 있는가
