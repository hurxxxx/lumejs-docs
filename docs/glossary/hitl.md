---
title: HITL(Human-in-the-Loop)
---

[← 용어 사전](../glossary)

## 한 줄 정의
HITL(Human-in-the-Loop)는 AI 시스템의 주요 의사결정 지점에 **사람이 개입하여 검토·승인·수정**하도록 설계하는 운영 방식이다.

## 왜 중요한가(실무)
AI가 아무리 정확해도, 결과가 고객·환자·법적 의무에 영향을 미치는 순간에는 "기계가 알아서 했다"가 통하지 않는다. 실무에서 HITL가 필요한 이유는 세 가지로 압축된다.

첫째, **리스크 차단**이다. 의료 소견서, 계약서 초안, 고객 안내 메일처럼 오류 비용이 큰 업무에서는 AI 출력을 사람이 한 번 걸러야 사고를 막는다. AI가 95% 맞더라도 나머지 5%가 매출·신뢰·법적 책임에 직결될 수 있다.

둘째, **규제 대응**이다. EU AI Act, 국내 개인정보보호법 등은 고위험 AI에 대해 인간 감독(human oversight)을 명시적으로 요구한다. HITL 설계가 없으면 컴플라이언스 자체가 불가능하다.

셋째, **품질 개선 루프**다. 사람이 AI 출력을 수정할 때마다, 그 수정 이력이 쌓인다. 이 데이터는 프롬프트 개선, 파인튜닝, Eval 기준 갱신의 원료가 된다. HITL는 단순한 안전장치가 아니라, AI를 지속적으로 개선하는 피드백 메커니즘이다.

## 핵심 이론(직관)
### 1) 개입 지점 설계: 어디에서 사람이 끼어드는가
HITL의 핵심은 "모든 곳에 사람을 넣는 것"이 아니라, **리스크가 높은 지점에만 정확히 배치**하는 것이다. 일반적으로 세 가지 지점이 있다.
- **사전 승인(Pre-approval)**: AI가 결과를 내놓으면, 배포 전에 사람이 승인한다 (예: 대외 문서 발송 전 검토).
- **예외 처리(Exception handling)**: AI 신뢰도가 임계치 이하일 때만 사람에게 넘긴다 (예: 분류 확신도 70% 미만이면 수동 처리).
- **주기적 감사(Periodic audit)**: 실시간이 아니라 주간/월간 단위로 샘플을 뽑아 검토한다.

### 2) 자동화와 감독의 균형
HITL를 과도하게 적용하면 병목이 된다. 모든 AI 출력에 승인을 붙이면 자동화의 의미가 사라진다. 반대로 너무 적으면 사고가 난다. 실무에서는 업무별로 **위험 등급(고/중/저)**을 나누고, 고위험에만 필수 승인, 중위험에는 샘플 감사, 저위험에는 로그 모니터링을 적용하는 것이 일반적이다.

### 3) 피드백 루프: 수정 이력이 곧 학습 데이터
사람이 AI 출력을 수정하면, 그 수정 전후 쌍(before/after)은 가장 값진 데이터다. 이를 체계적으로 수집하면 프롬프트 튜닝, Eval 데이터셋 구축, 파인튜닝에 직접 활용할 수 있다.

## 실무 포인트
### 1) HITL 워크플로우 설계 순서
1. 업무 목록에서 AI가 관여하는 단계를 모두 나열한다.
2. 각 단계의 오류 비용(재무·법적·평판)을 평가한다.
3. 고위험 단계에 승인/검토 게이트를 삽입한다.
4. 검토자의 역할·권한·SLA(응답 시간)를 RACI로 정의한다.
5. 수정 이력을 수집·저장하는 구조를 만든다.

### 2) 검토 피로(Review Fatigue) 방지
검토자가 하루에 수백 건을 승인해야 하면, 실질적인 검토 없이 "승인 클릭"만 반복하게 된다. 이를 방지하려면:
- AI 신뢰도 기반으로 검토 대상을 필터링한다 (전수가 아닌 표본).
- 검토 인터페이스를 간결하게 만든다 (AI 출력 + 근거 + 수정 버튼).
- 검토자 간 교차 검증(이중 검토)을 고위험 건에만 적용한다.

## 체크리스트
- AI가 관여하는 업무 중 고위험 단계를 식별했는가
- 각 고위험 단계에 승인/검토 게이트가 설계되어 있는가
- 검토자의 역할·권한·SLA가 명확히 정의되어 있는가
- 검토자 피로를 줄이기 위한 필터링/우선순위 로직이 있는가
- AI 출력 수정 이력을 체계적으로 수집·저장하고 있는가
- 수집된 수정 데이터를 프롬프트/모델 개선에 활용하는 프로세스가 있는가
- 규제 요건(EU AI Act, 개인정보보호법 등)에서 요구하는 인간 감독 수준을 충족하는가
